---
title: "Daily Notes: 2025-11-28"
date: 2025-11-28
categories: [daily]
---


ML Notes
- 


- 

```{python}
#| context: setup
#| echo: false
#| include: false
import sys
from pathlib import Path


def _ensure_code_on_path(marker="_quarto.yml", code_dir="code"):
    """Add the shared code directory to sys.path when rendering notes."""
    cwd = Path.cwd().resolve()
    for candidate in (cwd, *cwd.parents):
        if (candidate / marker).exists():
            code_path = candidate / code_dir
            if code_path.exists() and str(code_path) not in sys.path:
                sys.path.insert(0, str(code_path))
            return
    raise RuntimeError(f"Could not locate {marker} above {cwd}")


_ensure_code_on_path()
```

```{python}
#| label: iris-perceptron
#| cache: true
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

from ml_utils import Perceptron

s = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
df = pd.read_csv(s, header=None, encoding='utf-8') # dataframe

# print(df.tail()) # just to ensure that the data was loaded correctly

y = df.iloc[0:100, 4].values # creates a np array (.values) from the species labels (4 == 5th column == species) of the first 100 rows
y = np.where(y == 'Iris-setosa', 0, 1) # 0 if setosa, 1 if versicolor (first 100 rows only have those two species)

X = df.iloc[0:100, [0, 2]].values # 100 x 2 matrix that extracts two features: sepal length (0) and petal length (2)

# Plot the two iris classes in the 100 x 2 matrix to visualize how separable they are before fitting a perceptron
# It's important that it's roughly linearly separable, which is when a perceptron would be a good choice

plt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label = 'Setosa')
plt.scatter(X[50:100, 0], X[50:100, 1], color='blue', marker='s', label = 'Versicolor')

# Note that the x-axis here would be sepal length (because it's in column 0) and y-axis would be petal length (column 1)
plt.xlabel('Sepal length (cm)')
plt.ylabel('Petal length (cm)')
plt.legend(loc='upper left')
# plt.show()

ppn = Perceptron(eta=0.1, n_iter=10)
ppn.fit(X,y) # train Perceptron on the Iris data subset

# Explore how the perceptron's training errors evolve over epochs.
# range(1, len(ppn.errors_) + 1) just creates the epoch numbers (x-axis)
plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o') # draws a point for each training epoch
plt.xlabel('Epochs')
plt.ylabel('Number of updates')
plt.show()
```

::: {.callout-note title="Reusable code"}
This chunk pulls the `Perceptron` class from `code/ml_utils/perceptron.py`. Keeping reusable models in `code/` lets Quarto posts stay lean while GitHub readers can browse the source directly.
:::

Personal Notes
-
- I asked GPT-5.1 Codex to analyze my notes thus far and outline a path for what's to come:
    - Finish the perceptron loop: add dataset splits (train/test or cross-validation), compute accuracy/confusion matrices, and document failure cases when classes aren’t separable. This turns the current script into a minimal experiment you can evaluate.
    - Feature scaling & preprocessing: implement standardization pipelines to see how learning rate, convergence, and decision boundaries change, especially before moving beyond Iris.
    - Logistic regression & soft margins: move from perceptron to logistic regression or linear SVMs to understand probabilistic outputs, loss functions, and regularization—natural evolutions of your current model.
    - Multi-class strategies: prototype OvA or OvO wrappers around your perceptron/logistic models to reinforce how binary learners extend to multi-class tasks.
    - Experiment tracking: start logging hyperparameters, metrics, and plots (even in your daily notes) so you build the habit of reproducible ML experiments.
    - Deepen EDA intuition: answer your own question by comparing outcomes with and without exploratory plots—note when simple visuals caught issues early versus when they weren’t needed.


Questions I still have
- 
- 


Tomorrow's plan
- 
- 


