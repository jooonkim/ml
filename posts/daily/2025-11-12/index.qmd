---
title: "Daily ML Log â€” 2025-11-12"
categories: [daily]
date: 2025-11-12
---

## What I studied

- Logistic regression and the connection between odds and the sigmoid function.
- Gradient of the binary cross-entropy loss.

## Key formulas

For binary labels $y \in \{0,1\}$ and logit $z = w^\top x$:

$$
p(y=1 \mid x) = \sigma(z) = \frac{1}{1+e^{-z}}.
$$

The per-example loss:

$$
\ell(w) = - \big[ y \log \sigma(z) + (1-y) \log (1-\sigma(z)) \big].
$$

Gradient w.r.t. $w$:

$$
\nabla_w \ell(w) = (\sigma(z) - y) x.
$$

## Quick sanity check in code

```{python}
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

rng = np.random.default_rng(0)
X = rng.normal(size=(5, 3))
w = rng.normal(size=3)
y = (rng.random(5) < 0.4).astype(float)

z = X @ w
grad = ((sigmoid(z) - y)[:, None] * X).mean(axis=0)
grad
```

## Questions for tomorrow

- How does this generalize to multi-class (softmax) regression?
- What happens to the gradient when the model is very overconfident and wrong?
