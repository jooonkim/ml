---
title: "Daily Notes: 2025-11-16"
date: 2025-11-16
categories: [daily]
---


ML Notes
- 
Implementing the perceptron as a simple ML classification algorithm without scikit-learn

- **Mc-Culloch-Pitts (MCP) neuron model**: The biological neuron as a simple logic gate with multiple input signals arriving at the dendrites and a binary output.
- **Perceptron learning rule**: Frank Rosenblatt built upon this MCP neuron model by proposing an algorithm that would *learn* a weight vector $w$ that would be multiplied with the input features $x$ to make a decision about whether the neuron fires or not: i.e., a binary output.
- This is helpful because it can predict with the classification problem: does a new data point belong to one class or another?


Formally, a decision function $f(z)$ where, given a defined threshold $\theta$:

$z = w_1x_1 + w_2x_2 + ... + w_nx_n = w^Tx$

::: {.callout-note}
 $w$ and $z$ are both column vectors, which is why we take the **transpose** $w^T$ to get the **dot product** of the $(n \times 1)$ column vectors. $(1 \times n) * (n \times 1) = 1 \times 1$
:::

$f(z) = \begin{cases} 1, & z \ge \theta \\ 0, & z < \theta \end{cases}$

If we introduce a **bias unit** $b = -\theta$ for ease of implementation, then:

$z = w^Tx$ or $z = w_1x_1 + w_2x_2 + ... + w_nx_n + b = w^Tx + b$

$y = f(z) = \begin{cases} 1, & z \ge 0 \\ 0, & z < 0 \end{cases}$

The **perceptron learning rule** can be summarized as follows:

1. Initialize the weights and bias unit to 0
2. For each training example $x^{(i)}$, compute the output value $\hat{y}^{(i)}$ which is the **predicted class label** of the $i$th training example, predicted by the **threshold function** $f(z)$
3. Compare the **predicted class label** of the $i$th training example $\hat{y}^{(i)}$ to the **true class label** of the $i$th training example $y^{(i)}$
4. Update the weights $w$ and bias unit $b$ simultaneously

Formally,

$\forall w_j \in w, w_j := w_j + \Delta w_j$

$\Delta w_j = \eta(y^{(i)} - \hat{y}^{(i)})* x^{(i)}$

$b := b + \Delta b$

$\Delta b = \eta(y^{(i)} - \hat{y}^{(i)})$


::: {.callout-note}
$:=$ is "defined as"

$\eta$ is the Greek letter "eta" and is often used for the **learning rate** in ML, typically defined as a constant between 0 & 1
:::

Some observations:

- Each weight $w_j$ corresponds to a feature $x_j$. The bias unit $b$ does not.
- Each weight update $\Delta w_j$ is proportional to the value of $x^{(i)}_j$. The bias unit update is not. 
    - Compare $x^{(i)}_j = 10$ to $x^{(i)}_j = 1$ in the example where it is incorrectly classified as class $0$ when the true class label is $1$. Assume $\eta = 1$. 
    - $\Delta w_j = (1 - 0) * 10 = 10$
    - $\Delta w_j = (1 - 0) * 1 = 1$. 
    - The first example will push the decision boundary by a factor of $10$
- The bias unit $b$ is part of the linear combination (the score that the perceptron computes), not the activation (the **step function**). So, $y = f(z)$ is correct, not $y = f(z) + b$. The bias shifts the **decision boundary**. 
- You can see from the formal definition that the bias unit and weights remain unchanged when the perceptron predicts the class label correctly. *The perceptron only updates when it makes a mistake in classification.*
- If the data is **linearly separable**, the perceptron is guaranteed to find a **separating hyperplane** within a finite amount of updates. If not **linearly separable**, it will update forever - you need to maximum number of **epochs** in this situation.

::: {.callout-note}
A **step function** (in the context of perceptrons) is an activation function that outputs only two possible values. It decides yes/no based on whether the input crosses a threshold.

An **activation function** is applied to a neuron to determine whether it should "fire" or stay inactive.

An **epoch** is a pass over the training dataset.
:::

Implementation in Python

- If you define the perceptron interface as a Python class, you can initialize new `Perceptron` objects that can learn from data using a `fit` method and make predictions using a `predict` method.

::: {.callout-note}
An underscore _ is appended to attributes that are not created upon initialization of object, e.g., `self.w_` 

In Python's OOP framework, a **class** is the blueprint, an **object** is an instance of the class, `__init__` is the **initializer** method. An **instance method** is a function defined inside a class that operates on a specific instance (object) of that class.

Every instance method must take `self` as the first parameter because you need to explicitly state which object you are applying it to, i.e., `self.something` means apply this `something` to *this* object so it **persists**. Persistence is important because after the method finishes, the object will still "remember" it (you're attaching it to the object itself and can run `print(object.something)` on it after the method finishes).
:::

```{python}
#| eval: false

import numpy as np 

class Perceptron:
    """Perceptron classifier.

    Parameters

    eta: float, learning rate between 0.0 and 1.0
    n_iter: int, epochs
    random_state: int, random number generator for random weight initialization

    Attributes

    w_: 1d-array, weights after fitting
    b_: scalar, bias unit after fitting
    errors_: list, number of misclassifications aka updates in each epoch
    """

    def __init__(self, eta=0.01, n_iter = 20, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state

    def fit(self, X, y):
        """Fit training data.

        Parameters 

        X: array, features
        y: array, target values

        Returns

        self: object
        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])
        self.b_ = np.float_(0.)
        self.errors_ = []

        for _ in range(self.n_iter):
            errors = 0
            for xi, target in zip(X, y):
                update = self.eta * (target - self.predict(xi))
                self.w_ += update * xi
                self.b_ += update
                errors += int(update != 0.0)

        # Will finish implementation later

```

Personal Notes
-
- I first read about the MCP neuron model from *Why Machines Learn* by Anil Ananthaswamy and found his exposition to be helpful in understanding this chapter.
- I implemented all of these functions with Latex. I'm slowly getting the hang of it.

Questions I still have
- 
- N/A


Tomorrow's plan
- 
- 


