---
title: "Daily Notes: 2025-11-16"
date: 2025-11-16
categories: [daily]
---


ML Notes
- 
Implementing the perceptron as a simple ML classification algorithm without scikit-learn

- **Mc-Culloch-Pitts (MCP) neuron model**: The biological neuron as a simple logic gate with multiple input signals arriving at the dendrites and a binary output.
- **Perceptron learning rule**: Frank Rosenblatt built upon this MCP neuron model by proposing an algorithm that would *learn* a weight vector $w$ that would be multiplied with the input features $x$ to make a decision about whether the neuron fires or not: i.e., a binary output.
- This is helpful because it can predict with the classification problem: does a new data point belong to one class or another?


Formally, a decision function $f(z)$ where, given a defined threshold $\theta$:

$z = w_1x_1 + w_2x_2 + ... + w_nx_n = w^Tx$

::: {.callout-note}
 $w$ and $z$ are both column vectors, which is why we take the **transpose** $w^T$ to get the **dot product** of the $(n \times 1)$ column vectors. $(1 \times n) * (n \times 1) = 1 \times 1$
:::

$f(z) = \begin{cases} 1, & z \ge \theta \\ 0, & z < \theta \end{cases}$

If we introduce a **bias unit** $b = -\theta$, then:

$z = w^Tx$ or $z = w_1x_1 + w_2x_2 + ... + w_nx_n + b = w^Tx + b$

$f(z) = \begin{cases} 1, & z \ge 0 \\ 0, & z < 0 \end{cases}$

- This perceptron learning rule can be summarized as follows:
    - Initialize the weights and bias unit to 0
    - For each training example $x_i$, compute the output value $y_i$ which is basically the class label predicted by the function $f(z)$
    - Use $y_i$ to update the weights $w$ and bias unit $b$

Personal Notes
-
- I first read about the MCP neuron model from *Why Machines Learn* by Anil Ananthaswamy and found his exposition to be helpful in understanding this chapter.


Questions I still have
- 
- 


Tomorrow's plan
- 
- 


