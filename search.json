[
  {
    "objectID": "posts/daily/2025-11-16/index.html",
    "href": "posts/daily/2025-11-16/index.html",
    "title": "Daily Notes: 2025-11-16",
    "section": "",
    "text": "Implementing the perceptron as a simple ML classification algorithm without scikit-learn\n\nMc-Culloch-Pitts (MCP) neuron model: The biological neuron as a simple logic gate with multiple input signals arriving at the dendrites and a binary output.\nPerceptron learning rule: Frank Rosenblatt built upon this MCP neuron model by proposing an algorithm that would learn a weight vector \\(w\\) that would be multiplied with the input features \\(x\\) to make a decision about whether the neuron fires or not: i.e., a binary output.\nThis is helpful because it can predict with the classification problem: does a new data point belong to one class or another?\n\nFormally, a decision function \\(f(z)\\) where, given a defined threshold \\(\\theta\\):\n\\(z = w_1x_1 + w_2x_2 + ... + w_nx_n = w^Tx\\)\n\n\n\n\n\n\nNote\n\n\n\n\\(w\\) and \\(z\\) are both column vectors, which is why we take the transpose \\(w^T\\) to get the dot product of the \\((n \\times 1)\\) column vectors. \\((1 \\times n) * (n \\times 1) = 1 \\times 1\\)\n\n\n\\(f(z) = \\begin{cases} 1, & z \\ge \\theta \\\\ 0, & z &lt; \\theta \\end{cases}\\)\nIf we introduce a bias unit \\(b = -\\theta\\) for ease of implementation, then:\n\\(z = w^Tx\\) or \\(z = w_1x_1 + w_2x_2 + ... + w_nx_n + b = w^Tx + b\\)\n\\(y = f(z) = \\begin{cases} 1, & z \\ge 0 \\\\ 0, & z &lt; 0 \\end{cases}\\)\nThe perceptron learning rule can be summarized as follows:\n\nInitialize the weights and bias unit to 0\nFor each training example \\(x^{(i)}\\), compute the output value \\(\\hat{y}^{(i)}\\) which is the predicted class label of the \\(i\\)th training example, predicted by the threshold function \\(f(z)\\)\nCompare the predicted class label of the \\(i\\)th training example \\(\\hat{y}^{(i)}\\) to the true class label of the \\(i\\)th training example \\(y^{(i)}\\)\nUpdate the weights \\(w\\) and bias unit \\(b\\) simultaneously\n\nFormally,\n\\(\\forall w_j \\in w, w_j := w_j + \\Delta w_j\\)\n\\(\\Delta w_j = \\eta(y^{(i)} - \\hat{y}^{(i)})* x^{(i)}\\)\n\\(b := b + \\Delta b\\)\n\\(\\Delta b = \\eta(y^{(i)} - \\hat{y}^{(i)})\\)\n\n\n\n\n\n\nNote\n\n\n\n\\(:=\\) is “defined as”\n\\(\\eta\\) is the Greek letter “eta” and is often used for the learning rate in ML, typically defined as a constant between 0 & 1\n\n\nSome observations:\n\nEach weight \\(w_j\\) corresponds to a feature \\(x_j\\). The bias unit \\(b\\) does not.\nEach weight update \\(\\Delta w_j\\) is proportional to the value of \\(x^{(i)}_j\\). The bias unit update is not.\n\nCompare \\(x^{(i)}_j = 10\\) to \\(x^{(i)}_j = 1\\) in the example where it is incorrectly classified as class \\(0\\) when the true class label is \\(1\\). Assume \\(\\eta = 1\\)\n\n\\(\\Delta w_j = (1 - 0) * 10 = 10\\)\n\\(\\Delta w_j = (1 - 0) * 1 = 1\\)\n\nThe first example will push the decision boundary by a factor of \\(10\\)\n\nThe bias unit \\(b\\) is part of the linear combination (the score that the perceptron computes), not the activation (the step function). So, \\(y = f(z)\\) is correct, not \\(y = f(z) + b\\). The bias shifts the decision boundary.\nYou can see from the formal definition that the bias unit and weights remain unchanged when the perceptron predicts the class label correctly. The perceptron only updates when it makes a mistake in classification.\nIf the data is linearly separable, the perceptron is guaranteed to find a separating hyperplane within a finite amount of updates. If not linearly separable, it will update forever - you need to maximum number of epochs in this situation.\n\n\n\n\n\n\n\nNote\n\n\n\nA step function (in the context of perceptrons) is an activation function that outputs only two possible values. It decides yes/no based on whether the input crosses a threshold.\nAn activation function is applied to a neuron to determine whether it should “fire” or stay inactive.\nAn epoch is a pass over the training dataset.\n\n\nImplementation in Python\n\nIf you define the perceptron interface as a Python class, you can initialize new Perceptron objects that can learn from data using a fit method and make predictions using a predict method.\n\n\n\n\n\n\n\nNote\n\n\n\nAn underscore _ is appended to attributes that are not created upon initialization of object, e.g., self.w_\nIn Python’s OOP framework, a class is the blueprint, an object is an instance of the class, __init__ is the initializer method. An instance method is a function defined inside a class that operates on a specific instance (object) of that class.\nEvery instance method must take self as the first parameter because you need to explicitly state which object you are applying it to, i.e., self.something means apply this something to this object so it persists. Persistence is important because after the method finishes, the object will still “remember” it (you’re attaching it to the object itself and can run print(object.something) on it after the method finishes).\nStandard practice for any model class: set hyperparameters as instance attributes once on creation under __init__, then reuse them whenever you train or retrain the model.\n\n\n\nimport numpy as np \n\nclass Perceptron:\n    \"\"\"Perceptron classifier.\n\n    Parameters\n\n    eta: float, learning rate between 0.0 and 1.0\n    n_iter: int, epochs\n    random_state: int, random number generator (RNG) for random weight initialization\n\n    Attributes\n\n    w_: 1d-array, weights after fitting\n    b_: scalar, bias unit after fitting\n    errors_: list, number of misclassifications aka updates in each epoch\n    \"\"\"\n\n    def __init__(self, eta=0.01, n_iter = 20, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        \"\"\"Fit training data.\n\n        Parameters \n\n        X: array, features\n        y: array, target values\n\n        Returns\n\n        self: object\n        \"\"\"\n        rgen = np.random.RandomState(self.random_state) # random number generator\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1]) # initializes weight vector\n        # np.random.normal samples from a Gaussian (== normal distribution) with mean (loc = 0.0, no bias towards + or -) and std (scale = 0.01, weights begin near 0)\n        # size == the number of columns in X\n        self.b_ = np.float_(0.) # initializes bias value to 0.0\n        self.errors_ = [] # logs how many samples were misclassified at each epoch\n\n        for _ in range(self.n_iter): # repeats the training pass n_iter times\n            errors = 0 # resets counter at start of each epoch\n            for xi, target in zip(X, y): # zip pairs each feature vector xi from X with the corresponding target label in y\n                update = self.eta * (target - self.predict(xi)) # update will be 0 if there was no error\n                self.w_ += update * xi\n                self.b_ += update\n                errors += int(update != 0.0) # converts Boolean (update != 0.0) to 1 if True and 0 if False\n            self.errors_.append(errors)\n        return self\n    \n    def net_input(self, X):\n        \"\"\"Calculate net input\"\"\"\n        return np.dot(X, self.w_) + self.b_ # np.dot(a, b) is the dot product of a & b\n    \n    def predict(self, X):\n        \"\"\"Return class label\"\"\"\n        return np.where(self.net_input(X) &gt;= 0.0, 1, 0) # np.where(... &gt;= 0.0, 1, 0) returns 1 when the net input &gt;= 0.0, 0 otherwise\n\ndef test_perceptron_learns_and_gate():\n    # 1) Define the AND dataset\n    X = np.array([\n        [0, 0],\n        [0, 1],\n        [1, 0],\n        [1, 1],\n    ])\n    y = np.array([0, 0, 0, 1])\n\n    # 2) Create the model\n    clf = Perceptron(eta=0.1, n_iter=20, random_state=1)\n\n    # 3) Train on the dataset\n    clf.fit(X, y)\n\n    # 4) Check predictions\n    preds = clf.predict(X)\n\n    # 5) Assert predictions match the true labels\n    assert np.array_equal(preds, y)\n\nif __name__ == \"__main__\":\n    test_perceptron_learns_and_gate()\n    print(\"✅ Perceptron implementation passed.\")\n\n✅ Perceptron implementation passed."
  },
  {
    "objectID": "posts/daily/2025-11-16/index.html#ml-notes",
    "href": "posts/daily/2025-11-16/index.html#ml-notes",
    "title": "Daily Notes: 2025-11-16",
    "section": "",
    "text": "Implementing the perceptron as a simple ML classification algorithm without scikit-learn\n\nMc-Culloch-Pitts (MCP) neuron model: The biological neuron as a simple logic gate with multiple input signals arriving at the dendrites and a binary output.\nPerceptron learning rule: Frank Rosenblatt built upon this MCP neuron model by proposing an algorithm that would learn a weight vector \\(w\\) that would be multiplied with the input features \\(x\\) to make a decision about whether the neuron fires or not: i.e., a binary output.\nThis is helpful because it can predict with the classification problem: does a new data point belong to one class or another?\n\nFormally, a decision function \\(f(z)\\) where, given a defined threshold \\(\\theta\\):\n\\(z = w_1x_1 + w_2x_2 + ... + w_nx_n = w^Tx\\)\n\n\n\n\n\n\nNote\n\n\n\n\\(w\\) and \\(z\\) are both column vectors, which is why we take the transpose \\(w^T\\) to get the dot product of the \\((n \\times 1)\\) column vectors. \\((1 \\times n) * (n \\times 1) = 1 \\times 1\\)\n\n\n\\(f(z) = \\begin{cases} 1, & z \\ge \\theta \\\\ 0, & z &lt; \\theta \\end{cases}\\)\nIf we introduce a bias unit \\(b = -\\theta\\) for ease of implementation, then:\n\\(z = w^Tx\\) or \\(z = w_1x_1 + w_2x_2 + ... + w_nx_n + b = w^Tx + b\\)\n\\(y = f(z) = \\begin{cases} 1, & z \\ge 0 \\\\ 0, & z &lt; 0 \\end{cases}\\)\nThe perceptron learning rule can be summarized as follows:\n\nInitialize the weights and bias unit to 0\nFor each training example \\(x^{(i)}\\), compute the output value \\(\\hat{y}^{(i)}\\) which is the predicted class label of the \\(i\\)th training example, predicted by the threshold function \\(f(z)\\)\nCompare the predicted class label of the \\(i\\)th training example \\(\\hat{y}^{(i)}\\) to the true class label of the \\(i\\)th training example \\(y^{(i)}\\)\nUpdate the weights \\(w\\) and bias unit \\(b\\) simultaneously\n\nFormally,\n\\(\\forall w_j \\in w, w_j := w_j + \\Delta w_j\\)\n\\(\\Delta w_j = \\eta(y^{(i)} - \\hat{y}^{(i)})* x^{(i)}\\)\n\\(b := b + \\Delta b\\)\n\\(\\Delta b = \\eta(y^{(i)} - \\hat{y}^{(i)})\\)\n\n\n\n\n\n\nNote\n\n\n\n\\(:=\\) is “defined as”\n\\(\\eta\\) is the Greek letter “eta” and is often used for the learning rate in ML, typically defined as a constant between 0 & 1\n\n\nSome observations:\n\nEach weight \\(w_j\\) corresponds to a feature \\(x_j\\). The bias unit \\(b\\) does not.\nEach weight update \\(\\Delta w_j\\) is proportional to the value of \\(x^{(i)}_j\\). The bias unit update is not.\n\nCompare \\(x^{(i)}_j = 10\\) to \\(x^{(i)}_j = 1\\) in the example where it is incorrectly classified as class \\(0\\) when the true class label is \\(1\\). Assume \\(\\eta = 1\\)\n\n\\(\\Delta w_j = (1 - 0) * 10 = 10\\)\n\\(\\Delta w_j = (1 - 0) * 1 = 1\\)\n\nThe first example will push the decision boundary by a factor of \\(10\\)\n\nThe bias unit \\(b\\) is part of the linear combination (the score that the perceptron computes), not the activation (the step function). So, \\(y = f(z)\\) is correct, not \\(y = f(z) + b\\). The bias shifts the decision boundary.\nYou can see from the formal definition that the bias unit and weights remain unchanged when the perceptron predicts the class label correctly. The perceptron only updates when it makes a mistake in classification.\nIf the data is linearly separable, the perceptron is guaranteed to find a separating hyperplane within a finite amount of updates. If not linearly separable, it will update forever - you need to maximum number of epochs in this situation.\n\n\n\n\n\n\n\nNote\n\n\n\nA step function (in the context of perceptrons) is an activation function that outputs only two possible values. It decides yes/no based on whether the input crosses a threshold.\nAn activation function is applied to a neuron to determine whether it should “fire” or stay inactive.\nAn epoch is a pass over the training dataset.\n\n\nImplementation in Python\n\nIf you define the perceptron interface as a Python class, you can initialize new Perceptron objects that can learn from data using a fit method and make predictions using a predict method.\n\n\n\n\n\n\n\nNote\n\n\n\nAn underscore _ is appended to attributes that are not created upon initialization of object, e.g., self.w_\nIn Python’s OOP framework, a class is the blueprint, an object is an instance of the class, __init__ is the initializer method. An instance method is a function defined inside a class that operates on a specific instance (object) of that class.\nEvery instance method must take self as the first parameter because you need to explicitly state which object you are applying it to, i.e., self.something means apply this something to this object so it persists. Persistence is important because after the method finishes, the object will still “remember” it (you’re attaching it to the object itself and can run print(object.something) on it after the method finishes).\nStandard practice for any model class: set hyperparameters as instance attributes once on creation under __init__, then reuse them whenever you train or retrain the model.\n\n\n\nimport numpy as np \n\nclass Perceptron:\n    \"\"\"Perceptron classifier.\n\n    Parameters\n\n    eta: float, learning rate between 0.0 and 1.0\n    n_iter: int, epochs\n    random_state: int, random number generator (RNG) for random weight initialization\n\n    Attributes\n\n    w_: 1d-array, weights after fitting\n    b_: scalar, bias unit after fitting\n    errors_: list, number of misclassifications aka updates in each epoch\n    \"\"\"\n\n    def __init__(self, eta=0.01, n_iter = 20, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        \"\"\"Fit training data.\n\n        Parameters \n\n        X: array, features\n        y: array, target values\n\n        Returns\n\n        self: object\n        \"\"\"\n        rgen = np.random.RandomState(self.random_state) # random number generator\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1]) # initializes weight vector\n        # np.random.normal samples from a Gaussian (== normal distribution) with mean (loc = 0.0, no bias towards + or -) and std (scale = 0.01, weights begin near 0)\n        # size == the number of columns in X\n        self.b_ = np.float_(0.) # initializes bias value to 0.0\n        self.errors_ = [] # logs how many samples were misclassified at each epoch\n\n        for _ in range(self.n_iter): # repeats the training pass n_iter times\n            errors = 0 # resets counter at start of each epoch\n            for xi, target in zip(X, y): # zip pairs each feature vector xi from X with the corresponding target label in y\n                update = self.eta * (target - self.predict(xi)) # update will be 0 if there was no error\n                self.w_ += update * xi\n                self.b_ += update\n                errors += int(update != 0.0) # converts Boolean (update != 0.0) to 1 if True and 0 if False\n            self.errors_.append(errors)\n        return self\n    \n    def net_input(self, X):\n        \"\"\"Calculate net input\"\"\"\n        return np.dot(X, self.w_) + self.b_ # np.dot(a, b) is the dot product of a & b\n    \n    def predict(self, X):\n        \"\"\"Return class label\"\"\"\n        return np.where(self.net_input(X) &gt;= 0.0, 1, 0) # np.where(... &gt;= 0.0, 1, 0) returns 1 when the net input &gt;= 0.0, 0 otherwise\n\ndef test_perceptron_learns_and_gate():\n    # 1) Define the AND dataset\n    X = np.array([\n        [0, 0],\n        [0, 1],\n        [1, 0],\n        [1, 1],\n    ])\n    y = np.array([0, 0, 0, 1])\n\n    # 2) Create the model\n    clf = Perceptron(eta=0.1, n_iter=20, random_state=1)\n\n    # 3) Train on the dataset\n    clf.fit(X, y)\n\n    # 4) Check predictions\n    preds = clf.predict(X)\n\n    # 5) Assert predictions match the true labels\n    assert np.array_equal(preds, y)\n\nif __name__ == \"__main__\":\n    test_perceptron_learns_and_gate()\n    print(\"✅ Perceptron implementation passed.\")\n\n✅ Perceptron implementation passed."
  },
  {
    "objectID": "posts/daily/2025-11-16/index.html#personal-notes",
    "href": "posts/daily/2025-11-16/index.html#personal-notes",
    "title": "Daily Notes: 2025-11-16",
    "section": "Personal Notes",
    "text": "Personal Notes\n\nI first read about the MCP neuron model from Why Machines Learn by Anil Ananthaswamy and found his exposition to be helpful in understanding this chapter.\nI implemented all of these functions with Latex. I’m slowly getting the hang of it.\nI learned how to use pytest from my command line today. Note to self: pytest uses a discovery pattern of a filename starting with test_ I’m still a novice at writing tests and have been outsourcing this to AI. I understand that this is an important skill (even though LLMs are very good at this) so I should practice this…"
  },
  {
    "objectID": "posts/daily/2025-11-16/index.html#questions-i-still-have",
    "href": "posts/daily/2025-11-16/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-11-16",
    "section": "Questions I still have",
    "text": "Questions I still have\n\nN/A"
  },
  {
    "objectID": "posts/daily/2025-11-16/index.html#tomorrows-plan",
    "href": "posts/daily/2025-11-16/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-11-16",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan"
  },
  {
    "objectID": "posts/daily/2025-11-15/index.html",
    "href": "posts/daily/2025-11-15/index.html",
    "title": "Daily Notes: 2025-11-15",
    "section": "",
    "text": "A Mental Map for ML\n\nMachine Learning: Broad umbrella\nDeep Learning: Subset of ML using multi-layer neural networks\nTransformer architectures: Specific type of neural network architecture introduced in Attention Is All You Need (Vaswani et al., 2017).\nLLMs: a large transformer architecture, using a large corpus of text.\n\nThe importance of Linear Algebra\n\nLinear Algebra is the language of ML because ML is fundamentally the art of taking something (image, soundwave, text, etc.), representing it as a vector (or matrix), and creating a model that transforms that vector into another vector.\nAs an example, for a neural network layer, \\(y = Wx + b\\) represents \\(y\\) the output vector, \\(W\\) the learned weights (as a matrix), \\(x\\) the input vector, and \\(b\\) the bias vector.\nLinear Algebra permits us not only a compact representation, but also a language to reason about and manipulate these representations.\n\nThe importance of Probability\n\nThe guiding principle here is that some branches of computer science deal with entities that are deterministic and certain, but ML deals with entities that are stochastic (nondeterministic) and uncertain.\nProbability is how you reason in the presence of uncertainty. Interestingly, uncertainty is more common than we think… how many propositions are guaranteed to be true, or events are guaranteed to occur?\nThree possibile sources of uncertainty according to Goodfellow:\n\nInherent stochasticity in the system being modeled\nIncomplete observability: Even in a deterministic system, can’t observe all the variables that drive behavior (e.g., Monty Hall problem)\nIncomplete modeling: This one is the most interesting. A model must sometimes discard observed information. Sometimes, it’s more practical to use a simple & uncertain rule than a complex & certain & deterministic rule (e.g., “most birds fly” is better than “birds fly, except…”). It’s hard to develop, communicate, maintain, and make it resilient against failure.\n\n\nMiscellaneous\n\nSurprisingly, according to ChatGPT as an answer to one of my questions yesterday, Reinforcement Learning is typically “harder” than Unsupervised Learning.\n\nSupervised Learning is the “easiest” because you have clear labels, a clear objective, and a clear way of measuring success (goal: minimize loss).\nUnsupervised Learning is “conceptually tricky, computationally easy.” Yes, there’s no objective ground truth, but optimization is “stable” (i.e., the loss function is well-behaved, gradient descent makes progress reliably, feedback is consistent). Examples: PCA, k-means, etc.\nReinforcement Learning is hard because the environment is stochastic, the feedback is often delayed/sparse, and the optimization is unstable.\n\nTo answer another question from yesterday, here is the meaning of ““If you compute parameters for feature scaling or dimensionality reduction using all the data (train + test), the test performance becomes overly optimistic.”: your model has seen structures of the test data before evaluation, which means that there was leakage. The test set is supposed to simulate new, unseen, real-world data, and so it underestimates the “true error” that the model would have.\n\nRendering Python using Quarto…\n\nimport numpy as np \n\nx = np.arange(5)\nx\n\narray([0, 1, 2, 3, 4])"
  },
  {
    "objectID": "posts/daily/2025-11-15/index.html#ml-notes",
    "href": "posts/daily/2025-11-15/index.html#ml-notes",
    "title": "Daily Notes: 2025-11-15",
    "section": "",
    "text": "A Mental Map for ML\n\nMachine Learning: Broad umbrella\nDeep Learning: Subset of ML using multi-layer neural networks\nTransformer architectures: Specific type of neural network architecture introduced in Attention Is All You Need (Vaswani et al., 2017).\nLLMs: a large transformer architecture, using a large corpus of text.\n\nThe importance of Linear Algebra\n\nLinear Algebra is the language of ML because ML is fundamentally the art of taking something (image, soundwave, text, etc.), representing it as a vector (or matrix), and creating a model that transforms that vector into another vector.\nAs an example, for a neural network layer, \\(y = Wx + b\\) represents \\(y\\) the output vector, \\(W\\) the learned weights (as a matrix), \\(x\\) the input vector, and \\(b\\) the bias vector.\nLinear Algebra permits us not only a compact representation, but also a language to reason about and manipulate these representations.\n\nThe importance of Probability\n\nThe guiding principle here is that some branches of computer science deal with entities that are deterministic and certain, but ML deals with entities that are stochastic (nondeterministic) and uncertain.\nProbability is how you reason in the presence of uncertainty. Interestingly, uncertainty is more common than we think… how many propositions are guaranteed to be true, or events are guaranteed to occur?\nThree possibile sources of uncertainty according to Goodfellow:\n\nInherent stochasticity in the system being modeled\nIncomplete observability: Even in a deterministic system, can’t observe all the variables that drive behavior (e.g., Monty Hall problem)\nIncomplete modeling: This one is the most interesting. A model must sometimes discard observed information. Sometimes, it’s more practical to use a simple & uncertain rule than a complex & certain & deterministic rule (e.g., “most birds fly” is better than “birds fly, except…”). It’s hard to develop, communicate, maintain, and make it resilient against failure.\n\n\nMiscellaneous\n\nSurprisingly, according to ChatGPT as an answer to one of my questions yesterday, Reinforcement Learning is typically “harder” than Unsupervised Learning.\n\nSupervised Learning is the “easiest” because you have clear labels, a clear objective, and a clear way of measuring success (goal: minimize loss).\nUnsupervised Learning is “conceptually tricky, computationally easy.” Yes, there’s no objective ground truth, but optimization is “stable” (i.e., the loss function is well-behaved, gradient descent makes progress reliably, feedback is consistent). Examples: PCA, k-means, etc.\nReinforcement Learning is hard because the environment is stochastic, the feedback is often delayed/sparse, and the optimization is unstable.\n\nTo answer another question from yesterday, here is the meaning of ““If you compute parameters for feature scaling or dimensionality reduction using all the data (train + test), the test performance becomes overly optimistic.”: your model has seen structures of the test data before evaluation, which means that there was leakage. The test set is supposed to simulate new, unseen, real-world data, and so it underestimates the “true error” that the model would have.\n\nRendering Python using Quarto…\n\nimport numpy as np \n\nx = np.arange(5)\nx\n\narray([0, 1, 2, 3, 4])"
  },
  {
    "objectID": "posts/daily/2025-11-15/index.html#personal-notes",
    "href": "posts/daily/2025-11-15/index.html#personal-notes",
    "title": "Daily Notes: 2025-11-15",
    "section": "Personal Notes",
    "text": "Personal Notes\nI finally learned the differences between pip, Anaconda, and Miniconda today.\n\npip is a lightweight universal package installer that can install python libraries (e.g., numpy) pretty much anywhere. It only installs packages from PyPI.\nAnaconda is a (huge… 3-5 GB) Python distribution + environment manager - it bundles everything (pip, pre-compiled scientific packages, etc.) in one. Its scope goes beyond PyPI.\nMiniconda is a lighter version of Anaconda. It has the environment management that pip doesn’t have by default, and Python, but nothing else out the gate. This is important because you can manage your own Python versions in isolated environments.\nRecommendation: Use Miniconda for environment management + Python version control, use pip within each conda environment for packages.\nI created an environment using miniconda called pyml with the following packages: NumPy 1.21.2 SciPy 1.7.0 Scikit-learn 1.0 Matplotlib 3.4.3 pandas 1.3.2 python 3.9\n\nA “Karpathy-style” learning math on demand roadmap\n\nNote: Linear Algebra, followed by Probability, followed by Calculus are the most important\n\n\nStart with a tiny ML/DL problem first (e.g., train a linear classifier or a 2-layer neural net) You will immediately run into the following math concepts that need refreshing:\n\n\ngradients → calc\nmatrix multiplies → Lin Alg\nloss functions → probability + info theory\noptimization steps → calc + LA\nembeddings → SVD intuition\n\n\nWhen you start reading transformers papers, refresh only:\n\n\ndot products\nmatrix multiplications\nsoftmax derivatives\neigen decomposition (for understanding attention as soft nearest-neighbor search)\nprobability for next-token prediction\n\n\nWhen you study self-supervised methods, refresh only:\n\n\nKL divergence\ncross-entropy\ncovariance\nGaussians\ncontrastive loss geometry\n\n\nWhen you start fine-tuning LLMs, refresh only:\n\n\ngradients of logits\nJacobians\nbasic linear algebra on embeddings\nmatrix factorization intuition"
  },
  {
    "objectID": "posts/daily/2025-11-15/index.html#questions-i-still-have",
    "href": "posts/daily/2025-11-15/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-11-15",
    "section": "Questions I still have",
    "text": "Questions I still have\n\nN/A as of now"
  },
  {
    "objectID": "posts/daily/2025-11-15/index.html#tomorrows-plan",
    "href": "posts/daily/2025-11-15/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-11-15",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan\n\nActually implement something"
  },
  {
    "objectID": "posts/concepts/index.html",
    "href": "posts/concepts/index.html",
    "title": "Concepts",
    "section": "",
    "text": "More polished explanations of ML concepts that I want to be able to reference later.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/readings/index.html",
    "href": "posts/readings/index.html",
    "title": "Readings",
    "section": "",
    "text": "Notes from textbooks, blog posts, and research papers.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joon’s ML Notes",
    "section": "",
    "text": "Hello, World! This is my collection of ML notes & musings as I learn in public.\nYou can find the GitHub repository here."
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "Joon’s ML Notes",
    "section": "Latest Posts",
    "text": "Latest Posts\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-16\n\n\n\n\n\n\n\n\nNov 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-15\n\n\n\n\n\n\n\n\nNov 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-14\n\n\n\n\n\n\n\n\nNov 14, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/projects/index.html",
    "href": "posts/projects/index.html",
    "title": "Projects & Experiments",
    "section": "",
    "text": "Small experiments and projects: training models, reproducing papers, and playing with datasets.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/math/index.html",
    "href": "posts/math/index.html",
    "title": "Math Notes",
    "section": "",
    "text": "Derivations, proofs, and small math notes related to machine learning.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/daily/index.html",
    "href": "posts/daily/index.html",
    "title": "Daily ML Logs",
    "section": "",
    "text": "Short, lightly edited logs of what I worked on each day.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-16\n\n\n\n\n\n\n\n\nNov 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-15\n\n\n\n\n\n\n\n\nNov 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-14\n\n\n\n\n\n\n\n\nNov 14, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/daily/2025-11-14/index.html",
    "href": "posts/daily/2025-11-14/index.html",
    "title": "Daily Notes: 2025-11-14",
    "section": "",
    "text": "Three main types of ML.\n\nSupervised Learning, Unsupervised Learning, Reinforcement Learning\nSupervised Learning: Think of this as an approximation problem. Simplest way to distinguish the two types of Supervised Learning: Classification is a problem of discrete variables, Regression is a problem of continuous variables.\nUnsupervised Learning: Think of this as an extracting meaning problem. Given unlabeled data, what kinds of meaningful information can we extract without the guidance of a known outcome variable or measure of success?\nReinforcement Learning: This is basically supervised learning, but the feedback is not the correct ground truth. Rather, the feedback is a measure against a reward function. Central question: How do you maximize the (sometimes immediate, sometimes delayed) reward?\nUnsupervised Learning has some interesting subfields. One is clustering which can be called unsupervised classification, and another is dimensionality reduction which can be helpful in preprocessing to remove noise from data.\n\nMath Notation\n\nFeatures are \\(x\\) - think of this as variables, inputs, predictors, dimensions, etc.\nTargets are \\(y\\) - think of this as outcome, response variable, dependent variable, output, etc.\nTraining Examples are samples, instances, observations, etc. I prefer training example over sample because samples can also be used to refer to a collection of training examples.\nSuperscript \\(i\\) refers to the \\(i\\)th training example, and subscript \\(j\\) refers to the \\(j\\)th feature. Superscript on top, subscript on the bottom \\(x^i_j\\)\nVectors are \\(x \\in \\mathbb{R}^{n \\times 1}\\) and Matrices are \\(X \\in \\mathbb{R}^{n \\times m}\\)\n\\(x_{ij}\\) and \\(x^i_j\\) are both valid ways of writing the \\(j\\)th feature of the \\(i\\)th training example\nYou can think of every feature as an \\(i\\)-dimensional column vector \\(X^i \\in \\mathbb{R}^{i \\times 1}\\) (basically, think of a vertical vector)\n\nCross-Validation: The Answer to the Training vs. Testing vs. Validation problem\n\nIt makes sense to randomly divide a dataset into a training dataset and a testing dataset - you want to use the training data to train the model, and you want to use the testing dataset to evaluate the final model without any biases.\nHowever, we only know that the model performs well on the test data - we need to further validate it on real-world data (a validation dataset).\nCross-Validation allows us to further divide a datset into training and validation datasets to estimate the generalization performance of a model."
  },
  {
    "objectID": "posts/daily/2025-11-14/index.html#ml-notes",
    "href": "posts/daily/2025-11-14/index.html#ml-notes",
    "title": "Daily Notes: 2025-11-14",
    "section": "",
    "text": "Three main types of ML.\n\nSupervised Learning, Unsupervised Learning, Reinforcement Learning\nSupervised Learning: Think of this as an approximation problem. Simplest way to distinguish the two types of Supervised Learning: Classification is a problem of discrete variables, Regression is a problem of continuous variables.\nUnsupervised Learning: Think of this as an extracting meaning problem. Given unlabeled data, what kinds of meaningful information can we extract without the guidance of a known outcome variable or measure of success?\nReinforcement Learning: This is basically supervised learning, but the feedback is not the correct ground truth. Rather, the feedback is a measure against a reward function. Central question: How do you maximize the (sometimes immediate, sometimes delayed) reward?\nUnsupervised Learning has some interesting subfields. One is clustering which can be called unsupervised classification, and another is dimensionality reduction which can be helpful in preprocessing to remove noise from data.\n\nMath Notation\n\nFeatures are \\(x\\) - think of this as variables, inputs, predictors, dimensions, etc.\nTargets are \\(y\\) - think of this as outcome, response variable, dependent variable, output, etc.\nTraining Examples are samples, instances, observations, etc. I prefer training example over sample because samples can also be used to refer to a collection of training examples.\nSuperscript \\(i\\) refers to the \\(i\\)th training example, and subscript \\(j\\) refers to the \\(j\\)th feature. Superscript on top, subscript on the bottom \\(x^i_j\\)\nVectors are \\(x \\in \\mathbb{R}^{n \\times 1}\\) and Matrices are \\(X \\in \\mathbb{R}^{n \\times m}\\)\n\\(x_{ij}\\) and \\(x^i_j\\) are both valid ways of writing the \\(j\\)th feature of the \\(i\\)th training example\nYou can think of every feature as an \\(i\\)-dimensional column vector \\(X^i \\in \\mathbb{R}^{i \\times 1}\\) (basically, think of a vertical vector)\n\nCross-Validation: The Answer to the Training vs. Testing vs. Validation problem\n\nIt makes sense to randomly divide a dataset into a training dataset and a testing dataset - you want to use the training data to train the model, and you want to use the testing dataset to evaluate the final model without any biases.\nHowever, we only know that the model performs well on the test data - we need to further validate it on real-world data (a validation dataset).\nCross-Validation allows us to further divide a datset into training and validation datasets to estimate the generalization performance of a model."
  },
  {
    "objectID": "posts/daily/2025-11-14/index.html#personal-notes",
    "href": "posts/daily/2025-11-14/index.html#personal-notes",
    "title": "Daily Notes: 2025-11-14",
    "section": "Personal Notes",
    "text": "Personal Notes\n\nSebastian Raschka’s tweet (preserved below) is an incredible framework for how to read technical textbooks. When reading How to Read a Book by Mortimer J. Adler, I learned that non-fiction books and fiction books must be read in a different fashion. I now understand that technical books require a third approach. Too often, I find myself in “tutorial hell” where I take extensive notes on the first 1-2 chapters of any one book, and flit around from book to book. I also find that extensive notes in the beginning runs the risk of “missing the forest for the trees” with understanding what concepts are most important.\nI find Raschka’s writing style most compelling, so I will use Machine Learning with PyTorch and Scikit-Learn as my daily driver. I’m supplementing this with Tom Mitchell’s classic ML textbook (from the 1990s, but required by my course) and the Hands-On ML textbook by Aurélien Géron (there’s a newer PyTorch version of this book, but I’m finding it… hard to acquire).\nI understand that the most important part is not the qualitative note-taking, but the exercises and coding and deliberately breaking things. This is a difficult habit. I found it hard to get into Karpathy’s Zero to Hero and Jeremy Howard’s Fast AI courses for this reason. I hope that Raschka’s tweet will be a good framework.\nMy current study plan is measured by input rather than output - I am trying to hold myself accountable to 14 hours of input / week. I’m not measuring output at this stage because I’d like to allow myself random restarts (see the hill climbing problem). I am, however, forcing myself to continue to invest quality hours in a simple, measureable format.\nI really enjoyed watching these YouTube videos: Transformer Neural Networks by StatQuest and AI Engineering in 76 Minutes by Marina Wyss."
  },
  {
    "objectID": "posts/daily/2025-11-14/index.html#questions-i-still-have",
    "href": "posts/daily/2025-11-14/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-11-14",
    "section": "Questions I still have",
    "text": "Questions I still have\n\nMy grasp of the concept of unsupervised learning is admittedly still shakey. How do we know when we’re not grasping in the dark for a pattern that’s not there? Is this harder or easier than a typical SL or RL problem?\nMy understanding of the ML that everyone finds most exciting right now is ML -&gt; DL -&gt; Transformer Architectures -&gt; LLMs. What is the correct way to approach this topic?\nMath… as a former Math major I find it embarrassing how much Calculus/Lin Alg/Probability I have forgotten. I’d love to refresh but would also love to understand exactly what concepts are most required first. I’m resisting the urge to start Calculus 101 all over again so that I can apply Andrej Karpathy’s mantra of “learning on demand.”\nImportantly… parameters for techniques like feature scaling and dimensionality reduction are solely obtained from the training dataset… what does it mean that the performance measured on the test data is overly optimistic?"
  },
  {
    "objectID": "posts/daily/2025-11-14/index.html#tomorrows-plan",
    "href": "posts/daily/2025-11-14/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-11-14",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan\n\nContinue to read through Raschka."
  },
  {
    "objectID": "posts/daily/2025-11-14/index.html#addendum-raschkas-tweet",
    "href": "posts/daily/2025-11-14/index.html#addendum-raschkas-tweet",
    "title": "Daily Notes: 2025-11-14",
    "section": "Addendum: Raschka’s Tweet",
    "text": "Addendum: Raschka’s Tweet\n“I often get questions from readers about how to read and get the most out of my book(s) on building LLMs from scratch. My advice is usually based on how I read technical books myself. This is not a one-size-fits-all approach, but I thought it may be useful to share:\n\nRead the chapter preferably offline, away from the computer. Either classic physical form or at least on digital devices without internet. This really helps with focus time and minimizing distractions while reading. Highlighting or annotating confusing or interesting things is good, but I would not look things up at this stage. I also wouldn’t run code at this stage. At least not yet.\nOn the second read-through, type up and run the code from the chapter. Copying code is tempting because retyping is a lot of work, but it usually helps me to think about the code a bit more (versus just glancing over it). If I get different results than in the book, I would check the book’s GitHub repo and try the code from there. If I still get different results, I would try to see if it’s due to different package versions, random seeds, CPU/CUDA, etc. If I then still can’t find it out, asking the author would not be a bad idea (via book forum, public GitHub repo issues or discussions, and as a last resort, email)\nAfter the second read-through and retyping the code, it’s usually a good time to try the exercises to solidify my understanding. To check whether I actually understand the content and can work with it independently.\nGo through the highlights and annotations. I would bookmark important learnings or takeaways, if relevant for a given project, in my notes documents. Often, I also look up additional references to read more about a topic of interest. Also, if I still have any questions that I feel are unanswered after my previous readthroughs and exercises, I would do an online search to find out more.\nThe previous steps were all about soaking up knowledge. Eventually, though, I somehow want to use that knowledge. So I think about which projects would benefit from what I’ve learned and incorporate it into them. This could involve using the main concept from the chapter, but also sometimes minor tidbits I learned along the way, e.g., even trivial things like whether it actually makes a difference in my project to explicitly call torch.mps.manual_seed(seed) instead of just torch.manual_seed(seed).\n\nOf course, none of the above is set in stone. If the topic is overall very familiar or easy, and I am primarily reading the book to get some information in later chapters, skimming a chapter is ok (to not waste my time).\nAnyway, I hope this is useful. And happy reading and learning!”"
  }
]