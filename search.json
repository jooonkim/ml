[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joon Kim / Machine Learning Notes",
    "section": "",
    "text": "My collection of Machine Learning notes & musings as I learn in public."
  },
  {
    "objectID": "index.html#useful-links",
    "href": "index.html#useful-links",
    "title": "Joon Kim / Machine Learning Notes",
    "section": "Useful Links",
    "text": "Useful Links\n\nGitHub Repository\nPersonal Website"
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "Joon Kim / Machine Learning Notes",
    "section": "Latest Posts",
    "text": "Latest Posts\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-12-21\n\n\n\n\n\n\n\n\nDec 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-12-20\n\n\n\n\n\n\n\n\nDec 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-12-14\n\n\n\n\n\n\n\n\nDec 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-12-13\n\n\n\n\n\n\n\n\nDec 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-12-11\n\n\n\n\n\n\n\n\nDec 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-12-10\n\n\n\n\n\n\n\n\nDec 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-12-08\n\n\n\n\n\n\n\n\nDec 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-29\n\n\n\n\n\n\n\n\nNov 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-28\n\n\n\n\n\n\n\n\nNov 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-27\n\n\n\n\n\n\n\n\nNov 27, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/readings/index.html",
    "href": "posts/readings/index.html",
    "title": "Readings",
    "section": "",
    "text": "Notes from textbooks, blog posts, and research papers.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/math/index.html",
    "href": "posts/math/index.html",
    "title": "Math Notes",
    "section": "",
    "text": "Derivations, proofs, and small math notes related to machine learning.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/daily/2025-11-14/index.html",
    "href": "posts/daily/2025-11-14/index.html",
    "title": "Daily Notes: 2025-11-14",
    "section": "",
    "text": "Three main types of ML.\n\nSupervised Learning, Unsupervised Learning, Reinforcement Learning\nSupervised Learning: Think of this as an approximation problem. Simplest way to distinguish the two types of Supervised Learning: Classification is a problem of discrete variables, Regression is a problem of continuous variables.\nUnsupervised Learning: Think of this as an extracting meaning problem. Given unlabeled data, what kinds of meaningful information can we extract without the guidance of a known outcome variable or measure of success?\nReinforcement Learning: This is basically supervised learning, but the feedback is not the correct ground truth. Rather, the feedback is a measure against a reward function. Central question: How do you maximize the (sometimes immediate, sometimes delayed) reward?\nUnsupervised Learning has some interesting subfields. One is clustering which can be called unsupervised classification, and another is dimensionality reduction which can be helpful in preprocessing to remove noise from data.\n\nMath Notation\n\nFeatures are \\(x\\) - think of this as variables, inputs, predictors, dimensions, etc.\nTargets are \\(y\\) - think of this as outcome, response variable, dependent variable, output, etc.\nTraining Examples are samples, instances, observations, etc. I prefer training example over sample because samples can also be used to refer to a collection of training examples.\nSuperscript \\(i\\) refers to the \\(i\\)th training example, and subscript \\(j\\) refers to the \\(j\\)th feature. Superscript on top, subscript on the bottom \\(x^i_j\\)\nVectors are \\(x \\in \\mathbb{R}^{n \\times 1}\\) and Matrices are \\(X \\in \\mathbb{R}^{n \\times m}\\)\n\\(x_{ij}\\) and \\(x^i_j\\) are both valid ways of writing the \\(j\\)th feature of the \\(i\\)th training example\nYou can think of every feature as an \\(i\\)-dimensional column vector \\(X^i \\in \\mathbb{R}^{i \\times 1}\\) (basically, think of a vertical vector)\n\nCross-Validation: The Answer to the Training vs. Testing vs. Validation problem\n\nIt makes sense to randomly divide a dataset into a training dataset and a testing dataset - you want to use the training data to train the model, and you want to use the testing dataset to evaluate the final model without any biases.\nHowever, we only know that the model performs well on the test data - we need to further validate it on real-world data (a validation dataset).\nCross-Validation allows us to further divide a datset into training and validation datasets to estimate the generalization performance of a model."
  },
  {
    "objectID": "posts/daily/2025-11-14/index.html#ml-notes",
    "href": "posts/daily/2025-11-14/index.html#ml-notes",
    "title": "Daily Notes: 2025-11-14",
    "section": "",
    "text": "Three main types of ML.\n\nSupervised Learning, Unsupervised Learning, Reinforcement Learning\nSupervised Learning: Think of this as an approximation problem. Simplest way to distinguish the two types of Supervised Learning: Classification is a problem of discrete variables, Regression is a problem of continuous variables.\nUnsupervised Learning: Think of this as an extracting meaning problem. Given unlabeled data, what kinds of meaningful information can we extract without the guidance of a known outcome variable or measure of success?\nReinforcement Learning: This is basically supervised learning, but the feedback is not the correct ground truth. Rather, the feedback is a measure against a reward function. Central question: How do you maximize the (sometimes immediate, sometimes delayed) reward?\nUnsupervised Learning has some interesting subfields. One is clustering which can be called unsupervised classification, and another is dimensionality reduction which can be helpful in preprocessing to remove noise from data.\n\nMath Notation\n\nFeatures are \\(x\\) - think of this as variables, inputs, predictors, dimensions, etc.\nTargets are \\(y\\) - think of this as outcome, response variable, dependent variable, output, etc.\nTraining Examples are samples, instances, observations, etc. I prefer training example over sample because samples can also be used to refer to a collection of training examples.\nSuperscript \\(i\\) refers to the \\(i\\)th training example, and subscript \\(j\\) refers to the \\(j\\)th feature. Superscript on top, subscript on the bottom \\(x^i_j\\)\nVectors are \\(x \\in \\mathbb{R}^{n \\times 1}\\) and Matrices are \\(X \\in \\mathbb{R}^{n \\times m}\\)\n\\(x_{ij}\\) and \\(x^i_j\\) are both valid ways of writing the \\(j\\)th feature of the \\(i\\)th training example\nYou can think of every feature as an \\(i\\)-dimensional column vector \\(X^i \\in \\mathbb{R}^{i \\times 1}\\) (basically, think of a vertical vector)\n\nCross-Validation: The Answer to the Training vs. Testing vs. Validation problem\n\nIt makes sense to randomly divide a dataset into a training dataset and a testing dataset - you want to use the training data to train the model, and you want to use the testing dataset to evaluate the final model without any biases.\nHowever, we only know that the model performs well on the test data - we need to further validate it on real-world data (a validation dataset).\nCross-Validation allows us to further divide a datset into training and validation datasets to estimate the generalization performance of a model."
  },
  {
    "objectID": "posts/daily/2025-11-14/index.html#personal-notes",
    "href": "posts/daily/2025-11-14/index.html#personal-notes",
    "title": "Daily Notes: 2025-11-14",
    "section": "Personal Notes",
    "text": "Personal Notes\n\nSebastian Raschka’s tweet (preserved below) is an incredible framework for how to read technical textbooks. When reading How to Read a Book by Mortimer J. Adler, I learned that non-fiction books and fiction books must be read in a different fashion. I now understand that technical books require a third approach. Too often, I find myself in “tutorial hell” where I take extensive notes on the first 1-2 chapters of any one book, and flit around from book to book. I also find that extensive notes in the beginning runs the risk of “missing the forest for the trees” with understanding what concepts are most important.\nI find Raschka’s writing style most compelling, so I will use Machine Learning with PyTorch and Scikit-Learn as my daily driver. I’m supplementing this with Tom Mitchell’s classic ML textbook (from the 1990s, but required by my course) and the Hands-On ML textbook by Aurélien Géron (there’s a newer PyTorch version of this book, but I’m finding it… hard to acquire).\nI understand that the most important part is not the qualitative note-taking, but the exercises and coding and deliberately breaking things. This is a difficult habit. I found it hard to get into Karpathy’s Zero to Hero and Jeremy Howard’s Fast AI courses for this reason. I hope that Raschka’s tweet will be a good framework.\nMy current study plan is measured by input rather than output - I am trying to hold myself accountable to 14 hours of input / week. I’m not measuring output at this stage because I’d like to allow myself random restarts (see the hill climbing problem). I am, however, forcing myself to continue to invest quality hours in a simple, measureable format.\nI really enjoyed watching these YouTube videos: Transformer Neural Networks by StatQuest and AI Engineering in 76 Minutes by Marina Wyss."
  },
  {
    "objectID": "posts/daily/2025-11-14/index.html#questions-i-still-have",
    "href": "posts/daily/2025-11-14/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-11-14",
    "section": "Questions I still have",
    "text": "Questions I still have\n\nMy grasp of the concept of unsupervised learning is admittedly still shakey. How do we know when we’re not grasping in the dark for a pattern that’s not there? Is this harder or easier than a typical SL or RL problem?\nMy understanding of the ML that everyone finds most exciting right now is ML -&gt; DL -&gt; Transformer Architectures -&gt; LLMs. What is the correct way to approach this topic?\nMath… as a former Math major I find it embarrassing how much Calculus/Lin Alg/Probability I have forgotten. I’d love to refresh but would also love to understand exactly what concepts are most required first. I’m resisting the urge to start Calculus 101 all over again so that I can apply Andrej Karpathy’s mantra of “learning on demand.”\nImportantly… parameters for techniques like feature scaling and dimensionality reduction are solely obtained from the training dataset… what does it mean that the performance measured on the test data is overly optimistic?"
  },
  {
    "objectID": "posts/daily/2025-11-14/index.html#tomorrows-plan",
    "href": "posts/daily/2025-11-14/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-11-14",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan\n\nContinue to read through Raschka."
  },
  {
    "objectID": "posts/daily/2025-11-14/index.html#addendum-raschkas-tweet",
    "href": "posts/daily/2025-11-14/index.html#addendum-raschkas-tweet",
    "title": "Daily Notes: 2025-11-14",
    "section": "Addendum: Raschka’s Tweet",
    "text": "Addendum: Raschka’s Tweet\n“I often get questions from readers about how to read and get the most out of my book(s) on building LLMs from scratch. My advice is usually based on how I read technical books myself. This is not a one-size-fits-all approach, but I thought it may be useful to share:\n\nRead the chapter preferably offline, away from the computer. Either classic physical form or at least on digital devices without internet. This really helps with focus time and minimizing distractions while reading. Highlighting or annotating confusing or interesting things is good, but I would not look things up at this stage. I also wouldn’t run code at this stage. At least not yet.\nOn the second read-through, type up and run the code from the chapter. Copying code is tempting because retyping is a lot of work, but it usually helps me to think about the code a bit more (versus just glancing over it). If I get different results than in the book, I would check the book’s GitHub repo and try the code from there. If I still get different results, I would try to see if it’s due to different package versions, random seeds, CPU/CUDA, etc. If I then still can’t find it out, asking the author would not be a bad idea (via book forum, public GitHub repo issues or discussions, and as a last resort, email)\nAfter the second read-through and retyping the code, it’s usually a good time to try the exercises to solidify my understanding. To check whether I actually understand the content and can work with it independently.\nGo through the highlights and annotations. I would bookmark important learnings or takeaways, if relevant for a given project, in my notes documents. Often, I also look up additional references to read more about a topic of interest. Also, if I still have any questions that I feel are unanswered after my previous readthroughs and exercises, I would do an online search to find out more.\nThe previous steps were all about soaking up knowledge. Eventually, though, I somehow want to use that knowledge. So I think about which projects would benefit from what I’ve learned and incorporate it into them. This could involve using the main concept from the chapter, but also sometimes minor tidbits I learned along the way, e.g., even trivial things like whether it actually makes a difference in my project to explicitly call torch.mps.manual_seed(seed) instead of just torch.manual_seed(seed).\n\nOf course, none of the above is set in stone. If the topic is overall very familiar or easy, and I am primarily reading the book to get some information in later chapters, skimming a chapter is ok (to not waste my time).\nAnyway, I hope this is useful. And happy reading and learning!”"
  },
  {
    "objectID": "posts/daily/2025-12-20/index.html",
    "href": "posts/daily/2025-12-20/index.html",
    "title": "Daily Notes: 2025-12-20",
    "section": "",
    "text": "Bias Variance & Regularization\nThe five-step process that the authors propose in the paper Prediction of Advertiser Churn for Google AdWords:\n\nSelect samples for analysis\nDefine churn and select features (potential explanatory variables)\nProcess data: transform features and impute missing values\n\nThe goal of the third step is to generate more discriminating/relevant features to predict churn. This can be done via linear or non-linear transformations. Some examples are PCA, LDA, data preprocessing.\n\nBuild predictive models\nEvaluate trained models\n\n\n\n\nScreenshot showing the model workflow\n\n\n\n\n\nUnderfitting: High Bias\nOverfitting: High Variance\nThink of bias in ML as a algorithm’s preconception of the shape of the model.\nThink of variance in ML as how much the model might change based on new data. For a regression model, if it’s overfit on your dataset, then new data will lead to a wildly different model.\nWorkflow: Fit an algorithm that’s “quick & dirty” then understand whether it’s high bias or high variance and improve it.\nRegression models & Classification models are both subject to bias variance.\n\n\n\n\nThe Linear Regression Objective Function (“Least Squares Cost Function”) is:\n\\(\\min_{\\theta}\\; \\frac{1}{2}\\sum_{i=1}^{m}\\left\\|y^{(i)}-\\theta^{T}x^{(i)}\\right\\|^{2}\\)\n\n\\(\\min_{\\theta}\\): Choose a \\(\\theta\\) that minimizes the total squared residual magnitude across all training examples.\n\\(\\frac{1}{2}\\) doesn’t change the minimizer, and is included for algebraic convenience (to get rid of the factor of 2 when differentiating a squared term)\n\\(\\sum_{i=1}^{m}\\) is the sum across \\(m\\) training examples\nWith linear regression, the predictor \\(\\hat{y}^{(i)}\\) is the dot product \\(\\theta^{T}x^{(i)}\\). This is the guess of the model. It is linear with the parameters in that \\(\\theta\\) only appears to the first power. This is very important! Optimization becomes harder (you can use gradient descent for local minima) with non-linear parameters.\nIf \\(y^{i} \\in \\mathbb{R}\\) (a scalar output), then the Euclidean norm \\(\\left\\| \\right\\|\\) is redundant (you can use \\((...)\\) instead).\nBut given that \\(y^{i} \\in \\mathbb{R}^{k}\\) (a vector output), then you need the Euclidean norm.\n\nTo add Regularization, you add a regularization term:\n\\(\\min_{\\theta}\\; \\frac{1}{2}\\sum_{i=1}^{m}\\left\\|y^{(i)}-\\theta^{T}x^{(i)}\\right\\|^{2} + \\lambda\\left\\|\\theta\\right\\|^{2}\\)\nSometimes you write multiply \\(\\lambda\\) by \\(\\frac{1}{2}\\) to make derivation easier:\n\\(\\min_{\\theta}\\; \\frac{1}{2}\\sum_{i=1}^{m}\\left\\|y^{(i)}-\\theta^{T}x^{(i)}\\right\\|^{2} + \\frac{\\lambda}{2}\\left\\|\\theta\\right\\|^{2}\\)\n\nThe advantage of adding a small \\(\\lambda\\) (say, \\(\\lambda = 1\\)) is that it penalizes \\(\\theta\\) from being too big. Therefore, it makes it harder for the learning algorithm to overfit the data.\nIf \\(\\lambda\\) is too big (say, \\(\\lambda = 1000\\)), however, you risk underfitting the data."
  },
  {
    "objectID": "posts/daily/2025-12-20/index.html#ml-notes",
    "href": "posts/daily/2025-12-20/index.html#ml-notes",
    "title": "Daily Notes: 2025-12-20",
    "section": "",
    "text": "Bias Variance & Regularization\nThe five-step process that the authors propose in the paper Prediction of Advertiser Churn for Google AdWords:\n\nSelect samples for analysis\nDefine churn and select features (potential explanatory variables)\nProcess data: transform features and impute missing values\n\nThe goal of the third step is to generate more discriminating/relevant features to predict churn. This can be done via linear or non-linear transformations. Some examples are PCA, LDA, data preprocessing.\n\nBuild predictive models\nEvaluate trained models\n\n\n\n\nScreenshot showing the model workflow\n\n\n\n\n\nUnderfitting: High Bias\nOverfitting: High Variance\nThink of bias in ML as a algorithm’s preconception of the shape of the model.\nThink of variance in ML as how much the model might change based on new data. For a regression model, if it’s overfit on your dataset, then new data will lead to a wildly different model.\nWorkflow: Fit an algorithm that’s “quick & dirty” then understand whether it’s high bias or high variance and improve it.\nRegression models & Classification models are both subject to bias variance.\n\n\n\n\nThe Linear Regression Objective Function (“Least Squares Cost Function”) is:\n\\(\\min_{\\theta}\\; \\frac{1}{2}\\sum_{i=1}^{m}\\left\\|y^{(i)}-\\theta^{T}x^{(i)}\\right\\|^{2}\\)\n\n\\(\\min_{\\theta}\\): Choose a \\(\\theta\\) that minimizes the total squared residual magnitude across all training examples.\n\\(\\frac{1}{2}\\) doesn’t change the minimizer, and is included for algebraic convenience (to get rid of the factor of 2 when differentiating a squared term)\n\\(\\sum_{i=1}^{m}\\) is the sum across \\(m\\) training examples\nWith linear regression, the predictor \\(\\hat{y}^{(i)}\\) is the dot product \\(\\theta^{T}x^{(i)}\\). This is the guess of the model. It is linear with the parameters in that \\(\\theta\\) only appears to the first power. This is very important! Optimization becomes harder (you can use gradient descent for local minima) with non-linear parameters.\nIf \\(y^{i} \\in \\mathbb{R}\\) (a scalar output), then the Euclidean norm \\(\\left\\| \\right\\|\\) is redundant (you can use \\((...)\\) instead).\nBut given that \\(y^{i} \\in \\mathbb{R}^{k}\\) (a vector output), then you need the Euclidean norm.\n\nTo add Regularization, you add a regularization term:\n\\(\\min_{\\theta}\\; \\frac{1}{2}\\sum_{i=1}^{m}\\left\\|y^{(i)}-\\theta^{T}x^{(i)}\\right\\|^{2} + \\lambda\\left\\|\\theta\\right\\|^{2}\\)\nSometimes you write multiply \\(\\lambda\\) by \\(\\frac{1}{2}\\) to make derivation easier:\n\\(\\min_{\\theta}\\; \\frac{1}{2}\\sum_{i=1}^{m}\\left\\|y^{(i)}-\\theta^{T}x^{(i)}\\right\\|^{2} + \\frac{\\lambda}{2}\\left\\|\\theta\\right\\|^{2}\\)\n\nThe advantage of adding a small \\(\\lambda\\) (say, \\(\\lambda = 1\\)) is that it penalizes \\(\\theta\\) from being too big. Therefore, it makes it harder for the learning algorithm to overfit the data.\nIf \\(\\lambda\\) is too big (say, \\(\\lambda = 1000\\)), however, you risk underfitting the data."
  },
  {
    "objectID": "posts/daily/2025-12-20/index.html#personal-notes",
    "href": "posts/daily/2025-12-20/index.html#personal-notes",
    "title": "Daily Notes: 2025-12-20",
    "section": "Personal Notes",
    "text": "Personal Notes\n\nSelf-directed projects are hard to understand where to begin, so I read two papers for examples of Classification: Prediction of Advertiser Churn for Google AdWords and Building Airbnb Categories with ML and Human-in-the-Loop.\nFor the AdWords paper, I found it interesting that the definition of “customer churn” was not well-defined.\nThe challenge with top-down learning is that groping around in the dark can be incredibly demotivating. Watching the Stanford lectures felt like a breath of fresh air. I was also able to focus more effectively than just openly learning. Seems like the optimal balance is to have both.\nUsed Andrew Ng’s lectures on data splits to understand how to split data for my project.\nUsed Chris Piech’s lecture to understand Logistic Regression."
  },
  {
    "objectID": "posts/daily/2025-12-20/index.html#questions-i-still-have",
    "href": "posts/daily/2025-12-20/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-12-20",
    "section": "Questions I still have",
    "text": "Questions I still have"
  },
  {
    "objectID": "posts/daily/2025-12-20/index.html#tomorrows-plan",
    "href": "posts/daily/2025-12-20/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-12-20",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan"
  },
  {
    "objectID": "posts/daily/2025-12-10/index.html",
    "href": "posts/daily/2025-12-10/index.html",
    "title": "Daily Notes: 2025-12-10",
    "section": "",
    "text": "Adaline implementation\n\nclass AdalineGD:\n    \"\"\"ADAptive LInear NEuron classifier.\n\n    Parameters\n    --------------\n\n    eta: float. \n        This is the learning rate (between 0.0 and 1.0)\n    n_iter: int\n        Passes over the training dataset\n    random_state: int\n        Random number generator seed for random weight generalization\n    \n    Attributes\n    ---------------\n    w_: 1D-Array\n        Weights after fitting\n    b_: Scalar\n        Bias after fitting\n    losses_: list\n        Mean Squared Error loss function values in each epoch\n    \"\"\"\n    def __init__(self, eta=0.01, n_iter = 50, random_state = 1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def net_input(self, X):\n        \"\"\"Calculate net input\n        \n        Think of this as the linear combination of features and weights + bias\n        This helps get the neuron's \"raw score\"\n        \"\"\"\n        return np.dot(X, self.w_) + self.b_\n\n    def activation(self, X):\n        \"\"\"Compute linear activation\n        \n        This is just the identity.\"\"\"\n        return X\n    \n    def predict(self, X): \n        \"\"\"Return class label after unit step\n        \n        Binary class label of 1 when the activation &gt;= 0.5, else 0\n        \"\"\"\n        return np.where(self.activation(self.net_input(X)) &gt;= 0.5, 1, 0)\n\n    def fit(self, X, y):\n        \"\"\" Fit training data.\n\n        Parameters\n        -----------\n        X: array-like, shape = [m_examples, n_features]\n            Training vectors\n            m_examples: # of examples\n            n_features: # of features\n        \n        y: array-like, shape = [m_examples]\n            Target values\n        \n        Returns\n        -----------\n        self: object\n        \"\"\"\n\n        # Initialize the RNG, weights w_ with small random values, bias b_ to zero, losses_ empty\n        rgen = np.random.RandomState(self.random_state)\n        # This starts weights as tiny random numbers\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1]) \n        # This starts the bias at zero\n        self.b_ = np.float_(0.)\n        # This prepares a list to record training error at each pass\n        self.losses_ = []\n\n        for i in range(self.n_iter): # for each epoch\n            # Given whatever is w_ and b_, we're computing the raw score for each example\n            net_input = self.net_input(X) \n            # Activation is the identity (no \"squashing\" yet)\n            output = self.activation(net_input)\n            # How far off are we from the desired targets? \n            # Positive if too low, negative if too high\n            errors = (y - output)\n            \n            \"\"\"Weight update. \n            \n            This step is important!\n            We are calculating the gradient based on the whole training set,\n            not just evaluating each individual training example (as in the perceptron).\n            This makes the learning \"smoother\" - less aberrations because of individual examples.\n\n            This is called \"Batch Gradient Descent.\" \n            \n            Think of the model as making guesses with two kinds of knobs:\n            w_: one knob per input feature (like volume sliders for each input).\n            b_: one extra knob that shifts everything up or down (like a master volume).\n\n            Weight update (the \"averaged nudge\"):\n\n            Recall that error = (y - (X * w + b))\n            Multiplying by * 2.0 is because of the Chain Rule\n            The loss is error^2, so the derivative is 2 * error * derivative of error\n            Derivative of (X * w + b) w.r.t. w is X.\n            Multiply X.T * error to aggregate per feature across all samples\n\n            X has rows of training examples, columns of features\n            errors is rows of how wrong we were per example\n            X.T is the transpose of X so that each feature lines up with the \n                errors across example\n            X.T.dot(errors) is the dot product that combines every feature with \n                its errors. \n            X is (n_samples, n_features); errors is (n_samples,). \n            Flipping X gives X.T as (n_features, n_samples).\n            Dotting (n_features, n_samples) with (n_samples,) yields (n_features,):\n                a separate summed value for each feature.\n            \n            Recall that the loss is the mean squared error\n            Therefore, we need to divide by N (# of examples) so the \n                summed gradient becomes an average\n            / X.shape[0] means “take the average over all examples” so we dont \n                overreact to any single case.\n\n            Bias update: nudge the bias by the avg error\n            \"\"\"\n            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]\n            self.b_ += self.eta * 2.0 * errors.mean()\n            # Compute Mean Squared Error\n            loss = (errors**2).mean()\n            # Track loss history so we can see learning progress\n            self.losses_.append(loss)\n        return self"
  },
  {
    "objectID": "posts/daily/2025-12-10/index.html#ml-notes",
    "href": "posts/daily/2025-12-10/index.html#ml-notes",
    "title": "Daily Notes: 2025-12-10",
    "section": "",
    "text": "Adaline implementation\n\nclass AdalineGD:\n    \"\"\"ADAptive LInear NEuron classifier.\n\n    Parameters\n    --------------\n\n    eta: float. \n        This is the learning rate (between 0.0 and 1.0)\n    n_iter: int\n        Passes over the training dataset\n    random_state: int\n        Random number generator seed for random weight generalization\n    \n    Attributes\n    ---------------\n    w_: 1D-Array\n        Weights after fitting\n    b_: Scalar\n        Bias after fitting\n    losses_: list\n        Mean Squared Error loss function values in each epoch\n    \"\"\"\n    def __init__(self, eta=0.01, n_iter = 50, random_state = 1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def net_input(self, X):\n        \"\"\"Calculate net input\n        \n        Think of this as the linear combination of features and weights + bias\n        This helps get the neuron's \"raw score\"\n        \"\"\"\n        return np.dot(X, self.w_) + self.b_\n\n    def activation(self, X):\n        \"\"\"Compute linear activation\n        \n        This is just the identity.\"\"\"\n        return X\n    \n    def predict(self, X): \n        \"\"\"Return class label after unit step\n        \n        Binary class label of 1 when the activation &gt;= 0.5, else 0\n        \"\"\"\n        return np.where(self.activation(self.net_input(X)) &gt;= 0.5, 1, 0)\n\n    def fit(self, X, y):\n        \"\"\" Fit training data.\n\n        Parameters\n        -----------\n        X: array-like, shape = [m_examples, n_features]\n            Training vectors\n            m_examples: # of examples\n            n_features: # of features\n        \n        y: array-like, shape = [m_examples]\n            Target values\n        \n        Returns\n        -----------\n        self: object\n        \"\"\"\n\n        # Initialize the RNG, weights w_ with small random values, bias b_ to zero, losses_ empty\n        rgen = np.random.RandomState(self.random_state)\n        # This starts weights as tiny random numbers\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1]) \n        # This starts the bias at zero\n        self.b_ = np.float_(0.)\n        # This prepares a list to record training error at each pass\n        self.losses_ = []\n\n        for i in range(self.n_iter): # for each epoch\n            # Given whatever is w_ and b_, we're computing the raw score for each example\n            net_input = self.net_input(X) \n            # Activation is the identity (no \"squashing\" yet)\n            output = self.activation(net_input)\n            # How far off are we from the desired targets? \n            # Positive if too low, negative if too high\n            errors = (y - output)\n            \n            \"\"\"Weight update. \n            \n            This step is important!\n            We are calculating the gradient based on the whole training set,\n            not just evaluating each individual training example (as in the perceptron).\n            This makes the learning \"smoother\" - less aberrations because of individual examples.\n\n            This is called \"Batch Gradient Descent.\" \n            \n            Think of the model as making guesses with two kinds of knobs:\n            w_: one knob per input feature (like volume sliders for each input).\n            b_: one extra knob that shifts everything up or down (like a master volume).\n\n            Weight update (the \"averaged nudge\"):\n\n            Recall that error = (y - (X * w + b))\n            Multiplying by * 2.0 is because of the Chain Rule\n            The loss is error^2, so the derivative is 2 * error * derivative of error\n            Derivative of (X * w + b) w.r.t. w is X.\n            Multiply X.T * error to aggregate per feature across all samples\n\n            X has rows of training examples, columns of features\n            errors is rows of how wrong we were per example\n            X.T is the transpose of X so that each feature lines up with the \n                errors across example\n            X.T.dot(errors) is the dot product that combines every feature with \n                its errors. \n            X is (n_samples, n_features); errors is (n_samples,). \n            Flipping X gives X.T as (n_features, n_samples).\n            Dotting (n_features, n_samples) with (n_samples,) yields (n_features,):\n                a separate summed value for each feature.\n            \n            Recall that the loss is the mean squared error\n            Therefore, we need to divide by N (# of examples) so the \n                summed gradient becomes an average\n            / X.shape[0] means “take the average over all examples” so we dont \n                overreact to any single case.\n\n            Bias update: nudge the bias by the avg error\n            \"\"\"\n            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]\n            self.b_ += self.eta * 2.0 * errors.mean()\n            # Compute Mean Squared Error\n            loss = (errors**2).mean()\n            # Track loss history so we can see learning progress\n            self.losses_.append(loss)\n        return self"
  },
  {
    "objectID": "posts/daily/2025-12-10/index.html#personal-notes",
    "href": "posts/daily/2025-12-10/index.html#personal-notes",
    "title": "Daily Notes: 2025-12-10",
    "section": "Personal Notes",
    "text": "Personal Notes\n\nI think I understand the implementation line by line. This tweet by GabrielPeterss4 helped. It’s worth reviewing again and again."
  },
  {
    "objectID": "posts/daily/2025-12-10/index.html#questions-i-still-have",
    "href": "posts/daily/2025-12-10/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-12-10",
    "section": "Questions I still have",
    "text": "Questions I still have\n\nNeed to be able to implement this from scratch. I wonder if I’m missing the forest for the trees here, but I do think it’s important to really understand Gradient Descent forwards and backwards."
  },
  {
    "objectID": "posts/daily/2025-12-10/index.html#tomorrows-plan",
    "href": "posts/daily/2025-12-10/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-12-10",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan"
  },
  {
    "objectID": "posts/daily/2025-11-16/index.html",
    "href": "posts/daily/2025-11-16/index.html",
    "title": "Daily Notes: 2025-11-16",
    "section": "",
    "text": "Implementing the perceptron as a simple ML classification algorithm without scikit-learn\n\nMc-Culloch-Pitts (MCP) neuron model: The biological neuron as a simple logic gate with multiple input signals arriving at the dendrites and a binary output.\nPerceptron learning rule: Frank Rosenblatt built upon this MCP neuron model by proposing an algorithm that would learn a weight vector \\(w\\) that would be multiplied with the input features \\(x\\) to make a decision about whether the neuron fires or not: i.e., a binary output.\nThis is helpful because it can predict with the classification problem: does a new data point belong to one class or another?\n\nFormally, a decision function \\(f(z)\\) where, given a defined threshold \\(\\theta\\):\n\\(z = w_1x_1 + w_2x_2 + ... + w_nx_n = w^Tx\\)\n\n\n\n\n\n\nNote\n\n\n\n\\(w\\) and \\(z\\) are both column vectors, which is why we take the transpose \\(w^T\\) to get the dot product of the \\((n \\times 1)\\) column vectors. \\((1 \\times n) * (n \\times 1) = 1 \\times 1\\)\n\n\n\\(f(z) = \\begin{cases} 1, & z \\ge \\theta \\\\ 0, & z &lt; \\theta \\end{cases}\\)\nIf we introduce a bias unit \\(b = -\\theta\\) for ease of implementation, then:\n\\(z = w^Tx\\) or \\(z = w_1x_1 + w_2x_2 + ... + w_nx_n + b = w^Tx + b\\)\n\\(y = f(z) = \\begin{cases} 1, & z \\ge 0 \\\\ 0, & z &lt; 0 \\end{cases}\\)\nThe perceptron learning rule can be summarized as follows:\n\nInitialize the weights and bias unit to 0\nFor each training example \\(x^{(i)}\\), compute the output value \\(\\hat{y}^{(i)}\\) which is the predicted class label of the \\(i\\)th training example, predicted by the threshold function \\(f(z)\\)\nCompare the predicted class label of the \\(i\\)th training example \\(\\hat{y}^{(i)}\\) to the true class label of the \\(i\\)th training example \\(y^{(i)}\\)\nUpdate the weights \\(w\\) and bias unit \\(b\\) simultaneously\n\nFormally,\n\\(\\forall w_j \\in w, w_j := w_j + \\Delta w_j\\)\n\\(\\Delta w_j = \\eta(y^{(i)} - \\hat{y}^{(i)})* x^{(i)}\\)\n\\(b := b + \\Delta b\\)\n\\(\\Delta b = \\eta(y^{(i)} - \\hat{y}^{(i)})\\)\n\n\n\n\n\n\nNote\n\n\n\n\\(:=\\) is “defined as”\n\\(\\eta\\) is the Greek letter “eta” and is often used for the learning rate in ML, typically defined as a constant between 0 & 1\n\n\nSome observations:\n\nEach weight \\(w_j\\) corresponds to a feature \\(x_j\\). The bias unit \\(b\\) does not.\nEach weight update \\(\\Delta w_j\\) is proportional to the value of \\(x^{(i)}_j\\). The bias unit update is not.\n\nCompare \\(x^{(i)}_j = 10\\) to \\(x^{(i)}_j = 1\\) in the example where it is incorrectly classified as class \\(0\\) when the true class label is \\(1\\). Assume \\(\\eta = 1\\)\n\n\\(\\Delta w_j = (1 - 0) * 10 = 10\\)\n\\(\\Delta w_j = (1 - 0) * 1 = 1\\)\n\nThe first example will push the decision boundary by a factor of \\(10\\)\n\nThe bias unit \\(b\\) is part of the linear combination (the score that the perceptron computes), not the activation (the step function). So, \\(y = f(z)\\) is correct, not \\(y = f(z) + b\\). The bias shifts the decision boundary.\nYou can see from the formal definition that the bias unit and weights remain unchanged when the perceptron predicts the class label correctly. The perceptron only updates when it makes a mistake in classification.\nIf the data is linearly separable, the perceptron is guaranteed to find a separating hyperplane within a finite amount of updates. If not linearly separable, it will update forever - you need to maximum number of epochs in this situation.\n\n\n\n\n\n\n\nNote\n\n\n\nA step function (in the context of perceptrons) is an activation function that outputs only two possible values. It decides yes/no based on whether the input crosses a threshold.\nAn activation function is applied to a neuron to determine whether it should “fire” or stay inactive.\nAn epoch is a pass over the training dataset.\n\n\nImplementation in Python\n\nIf you define the perceptron interface as a Python class, you can initialize new Perceptron objects that can learn from data using a fit method and make predictions using a predict method.\n\n\n\n\n\n\n\nNote\n\n\n\nAn underscore _ is appended to attributes that are not created upon initialization of object, e.g., self.w_\nIn Python’s OOP framework, a class is the blueprint, an object is an instance of the class, __init__ is the initializer method. An instance method is a function defined inside a class that operates on a specific instance (object) of that class.\nEvery instance method must take self as the first parameter because you need to explicitly state which object you are applying it to, i.e., self.something means apply this something to this object so it persists. Persistence is important because after the method finishes, the object will still “remember” it (you’re attaching it to the object itself and can run print(object.something) on it after the method finishes).\nStandard practice for any model class: set hyperparameters as instance attributes once on creation under __init__, then reuse them whenever you train or retrain the model.\n\n\n\nimport numpy as np \n\nclass Perceptron:\n    \"\"\"Perceptron classifier.\n\n    Parameters\n\n    eta: float, learning rate between 0.0 and 1.0\n    n_iter: int, epochs\n    random_state: int, random number generator (RNG) for random weight initialization\n\n    Attributes\n\n    w_: 1d-array, weights after fitting\n    b_: scalar, bias unit after fitting\n    errors_: list, number of misclassifications aka updates in each epoch\n    \"\"\"\n\n    def __init__(self, eta=0.01, n_iter = 20, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        \"\"\"Fit training data.\n\n        Parameters \n\n        X: array, features\n        y: array, target values\n\n        Returns\n\n        self: object\n        \"\"\"\n        rgen = np.random.RandomState(self.random_state) # random number generator\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1]) # initializes weight vector\n        # np.random.normal samples from a Gaussian (== normal distribution) with mean (loc = 0.0, no bias towards + or -) and std (scale = 0.01, weights begin near 0)\n        # size == the number of columns in X\n        self.b_ = np.float_(0.) # initializes bias value to 0.0\n        self.errors_ = [] # logs how many samples were misclassified at each epoch\n\n        for _ in range(self.n_iter): # repeats the training pass n_iter times\n            errors = 0 # resets counter at start of each epoch\n            for xi, target in zip(X, y): # zip pairs each feature vector xi from X with the corresponding target label in y\n                update = self.eta * (target - self.predict(xi)) # update will be 0 if there was no error\n                self.w_ += update * xi\n                self.b_ += update\n                errors += int(update != 0.0) # converts Boolean (update != 0.0) to 1 if True and 0 if False\n            self.errors_.append(errors)\n        return self\n    \n    def net_input(self, X):\n        \"\"\"Calculate net input\"\"\"\n        return np.dot(X, self.w_) + self.b_ # np.dot(a, b) is the dot product of a & b\n    \n    def predict(self, X):\n        \"\"\"Return class label\"\"\"\n        return np.where(self.net_input(X) &gt;= 0.0, 1, 0) # np.where(... &gt;= 0.0, 1, 0) returns 1 when the net input &gt;= 0.0, 0 otherwise\n\ndef test_perceptron_learns_and_gate():\n    # 1) Define the AND dataset\n    X = np.array([\n        [0, 0],\n        [0, 1],\n        [1, 0],\n        [1, 1],\n    ])\n    y = np.array([0, 0, 0, 1])\n\n    # 2) Create the model\n    clf = Perceptron(eta=0.1, n_iter=20, random_state=1)\n\n    # 3) Train on the dataset\n    clf.fit(X, y)\n\n    # 4) Check predictions\n    preds = clf.predict(X)\n\n    # 5) Assert predictions match the true labels\n    assert np.array_equal(preds, y)\n\nif __name__ == \"__main__\":\n    test_perceptron_learns_and_gate()\n    print(\"✅ Perceptron implementation passed.\")\n\n✅ Perceptron implementation passed."
  },
  {
    "objectID": "posts/daily/2025-11-16/index.html#ml-notes",
    "href": "posts/daily/2025-11-16/index.html#ml-notes",
    "title": "Daily Notes: 2025-11-16",
    "section": "",
    "text": "Implementing the perceptron as a simple ML classification algorithm without scikit-learn\n\nMc-Culloch-Pitts (MCP) neuron model: The biological neuron as a simple logic gate with multiple input signals arriving at the dendrites and a binary output.\nPerceptron learning rule: Frank Rosenblatt built upon this MCP neuron model by proposing an algorithm that would learn a weight vector \\(w\\) that would be multiplied with the input features \\(x\\) to make a decision about whether the neuron fires or not: i.e., a binary output.\nThis is helpful because it can predict with the classification problem: does a new data point belong to one class or another?\n\nFormally, a decision function \\(f(z)\\) where, given a defined threshold \\(\\theta\\):\n\\(z = w_1x_1 + w_2x_2 + ... + w_nx_n = w^Tx\\)\n\n\n\n\n\n\nNote\n\n\n\n\\(w\\) and \\(z\\) are both column vectors, which is why we take the transpose \\(w^T\\) to get the dot product of the \\((n \\times 1)\\) column vectors. \\((1 \\times n) * (n \\times 1) = 1 \\times 1\\)\n\n\n\\(f(z) = \\begin{cases} 1, & z \\ge \\theta \\\\ 0, & z &lt; \\theta \\end{cases}\\)\nIf we introduce a bias unit \\(b = -\\theta\\) for ease of implementation, then:\n\\(z = w^Tx\\) or \\(z = w_1x_1 + w_2x_2 + ... + w_nx_n + b = w^Tx + b\\)\n\\(y = f(z) = \\begin{cases} 1, & z \\ge 0 \\\\ 0, & z &lt; 0 \\end{cases}\\)\nThe perceptron learning rule can be summarized as follows:\n\nInitialize the weights and bias unit to 0\nFor each training example \\(x^{(i)}\\), compute the output value \\(\\hat{y}^{(i)}\\) which is the predicted class label of the \\(i\\)th training example, predicted by the threshold function \\(f(z)\\)\nCompare the predicted class label of the \\(i\\)th training example \\(\\hat{y}^{(i)}\\) to the true class label of the \\(i\\)th training example \\(y^{(i)}\\)\nUpdate the weights \\(w\\) and bias unit \\(b\\) simultaneously\n\nFormally,\n\\(\\forall w_j \\in w, w_j := w_j + \\Delta w_j\\)\n\\(\\Delta w_j = \\eta(y^{(i)} - \\hat{y}^{(i)})* x^{(i)}\\)\n\\(b := b + \\Delta b\\)\n\\(\\Delta b = \\eta(y^{(i)} - \\hat{y}^{(i)})\\)\n\n\n\n\n\n\nNote\n\n\n\n\\(:=\\) is “defined as”\n\\(\\eta\\) is the Greek letter “eta” and is often used for the learning rate in ML, typically defined as a constant between 0 & 1\n\n\nSome observations:\n\nEach weight \\(w_j\\) corresponds to a feature \\(x_j\\). The bias unit \\(b\\) does not.\nEach weight update \\(\\Delta w_j\\) is proportional to the value of \\(x^{(i)}_j\\). The bias unit update is not.\n\nCompare \\(x^{(i)}_j = 10\\) to \\(x^{(i)}_j = 1\\) in the example where it is incorrectly classified as class \\(0\\) when the true class label is \\(1\\). Assume \\(\\eta = 1\\)\n\n\\(\\Delta w_j = (1 - 0) * 10 = 10\\)\n\\(\\Delta w_j = (1 - 0) * 1 = 1\\)\n\nThe first example will push the decision boundary by a factor of \\(10\\)\n\nThe bias unit \\(b\\) is part of the linear combination (the score that the perceptron computes), not the activation (the step function). So, \\(y = f(z)\\) is correct, not \\(y = f(z) + b\\). The bias shifts the decision boundary.\nYou can see from the formal definition that the bias unit and weights remain unchanged when the perceptron predicts the class label correctly. The perceptron only updates when it makes a mistake in classification.\nIf the data is linearly separable, the perceptron is guaranteed to find a separating hyperplane within a finite amount of updates. If not linearly separable, it will update forever - you need to maximum number of epochs in this situation.\n\n\n\n\n\n\n\nNote\n\n\n\nA step function (in the context of perceptrons) is an activation function that outputs only two possible values. It decides yes/no based on whether the input crosses a threshold.\nAn activation function is applied to a neuron to determine whether it should “fire” or stay inactive.\nAn epoch is a pass over the training dataset.\n\n\nImplementation in Python\n\nIf you define the perceptron interface as a Python class, you can initialize new Perceptron objects that can learn from data using a fit method and make predictions using a predict method.\n\n\n\n\n\n\n\nNote\n\n\n\nAn underscore _ is appended to attributes that are not created upon initialization of object, e.g., self.w_\nIn Python’s OOP framework, a class is the blueprint, an object is an instance of the class, __init__ is the initializer method. An instance method is a function defined inside a class that operates on a specific instance (object) of that class.\nEvery instance method must take self as the first parameter because you need to explicitly state which object you are applying it to, i.e., self.something means apply this something to this object so it persists. Persistence is important because after the method finishes, the object will still “remember” it (you’re attaching it to the object itself and can run print(object.something) on it after the method finishes).\nStandard practice for any model class: set hyperparameters as instance attributes once on creation under __init__, then reuse them whenever you train or retrain the model.\n\n\n\nimport numpy as np \n\nclass Perceptron:\n    \"\"\"Perceptron classifier.\n\n    Parameters\n\n    eta: float, learning rate between 0.0 and 1.0\n    n_iter: int, epochs\n    random_state: int, random number generator (RNG) for random weight initialization\n\n    Attributes\n\n    w_: 1d-array, weights after fitting\n    b_: scalar, bias unit after fitting\n    errors_: list, number of misclassifications aka updates in each epoch\n    \"\"\"\n\n    def __init__(self, eta=0.01, n_iter = 20, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        \"\"\"Fit training data.\n\n        Parameters \n\n        X: array, features\n        y: array, target values\n\n        Returns\n\n        self: object\n        \"\"\"\n        rgen = np.random.RandomState(self.random_state) # random number generator\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1]) # initializes weight vector\n        # np.random.normal samples from a Gaussian (== normal distribution) with mean (loc = 0.0, no bias towards + or -) and std (scale = 0.01, weights begin near 0)\n        # size == the number of columns in X\n        self.b_ = np.float_(0.) # initializes bias value to 0.0\n        self.errors_ = [] # logs how many samples were misclassified at each epoch\n\n        for _ in range(self.n_iter): # repeats the training pass n_iter times\n            errors = 0 # resets counter at start of each epoch\n            for xi, target in zip(X, y): # zip pairs each feature vector xi from X with the corresponding target label in y\n                update = self.eta * (target - self.predict(xi)) # update will be 0 if there was no error\n                self.w_ += update * xi\n                self.b_ += update\n                errors += int(update != 0.0) # converts Boolean (update != 0.0) to 1 if True and 0 if False\n            self.errors_.append(errors)\n        return self\n    \n    def net_input(self, X):\n        \"\"\"Calculate net input\"\"\"\n        return np.dot(X, self.w_) + self.b_ # np.dot(a, b) is the dot product of a & b\n    \n    def predict(self, X):\n        \"\"\"Return class label\"\"\"\n        return np.where(self.net_input(X) &gt;= 0.0, 1, 0) # np.where(... &gt;= 0.0, 1, 0) returns 1 when the net input &gt;= 0.0, 0 otherwise\n\ndef test_perceptron_learns_and_gate():\n    # 1) Define the AND dataset\n    X = np.array([\n        [0, 0],\n        [0, 1],\n        [1, 0],\n        [1, 1],\n    ])\n    y = np.array([0, 0, 0, 1])\n\n    # 2) Create the model\n    clf = Perceptron(eta=0.1, n_iter=20, random_state=1)\n\n    # 3) Train on the dataset\n    clf.fit(X, y)\n\n    # 4) Check predictions\n    preds = clf.predict(X)\n\n    # 5) Assert predictions match the true labels\n    assert np.array_equal(preds, y)\n\nif __name__ == \"__main__\":\n    test_perceptron_learns_and_gate()\n    print(\"✅ Perceptron implementation passed.\")\n\n✅ Perceptron implementation passed."
  },
  {
    "objectID": "posts/daily/2025-11-16/index.html#personal-notes",
    "href": "posts/daily/2025-11-16/index.html#personal-notes",
    "title": "Daily Notes: 2025-11-16",
    "section": "Personal Notes",
    "text": "Personal Notes\n\nI first read about the MCP neuron model from Why Machines Learn by Anil Ananthaswamy and found his exposition to be helpful in understanding this chapter.\nI implemented all of these functions with Latex. I’m slowly getting the hang of it.\nI learned how to use pytest from my command line today. Note to self: pytest uses a discovery pattern of a filename starting with test_ I’m still a novice at writing tests and have been outsourcing this to AI. I understand that this is an important skill (even though LLMs are very good at this) so I should practice this…"
  },
  {
    "objectID": "posts/daily/2025-11-16/index.html#questions-i-still-have",
    "href": "posts/daily/2025-11-16/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-11-16",
    "section": "Questions I still have",
    "text": "Questions I still have\n\nN/A"
  },
  {
    "objectID": "posts/daily/2025-11-16/index.html#tomorrows-plan",
    "href": "posts/daily/2025-11-16/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-11-16",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan\n\nContinue with Chapter 2 of Raschka’s ML textbook."
  },
  {
    "objectID": "posts/daily/2025-11-27/index.html",
    "href": "posts/daily/2025-11-27/index.html",
    "title": "Daily Notes: 2025-11-27",
    "section": "",
    "text": "In which we test whether the perceptron converged and visualize the decision boundaries for this 2-D dataset.\n\nfrom matplotlib.colors import ListedColormap\nfrom ml_utils.perceptron import Perceptron\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ns = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\ndf = pd.read_csv(s, header=None, encoding='utf-8') # dataframe\n\n# print(df.tail()) # just to ensure that the data was loaded correctly\n\n# Goal here is to restrict the task to a binary classification problem\ny = df.iloc[0:100, 4].values # creates a np array (.values) from the species labels (4 == 5th column == species) of the first 100 rows\ny = np.where(y == 'Iris-setosa', 0, 1) # 0 if setosa, 1 if versicolor (first 100 rows only have those two species)\n\n# Goal here is to extract two fairly separable features from the same 100 rows of y\nX = df.iloc[0:100, [0, 2]].values # 100 x 2 matrix that extracts two features: sepal length (0) and petal length (2)\n\n# Plot the two iris classes in the 100 x 2 matrix to visualize how separable they are before fitting a perceptron\n# It's important that it's roughly linearly separable, which is when a perceptron would be a good choice\n\nplt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label = 'Setosa')\nplt.scatter(X[50:100, 0], X[50:100, 1], color='blue', marker='s', label = 'Versicolor')\n\n# Note that the x-axis here would be sepal length (because it's in column 0) and y-axis would be petal length (column 1)\nplt.xlabel('Sepal length (cm)')\nplt.ylabel('Petal length (cm)')\nplt.legend(loc='upper left')\nplt.show()\n\nppn = Perceptron(eta=0.1, n_iter=10)\nppn.fit(X,y) # train Perceptron on the Iris data subset\n\n# Explore how the perceptron's training errors evolve over epochs.\n# range(1, len(ppn.errors_) + 1) just creates the epoch numbers (x-axis)\nplt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o') # draws a point for each training epoch\nplt.xlabel('Epochs')\nplt.ylabel('Number of updates')\nplt.show() # Converges after 6th epoch\n\ndef plot_decision_regions(X, y, classifier, resolution=0.02):\n\n    # setup marker generator and color map\n    markers = ('o', 's', '^', 'v', '&lt;')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    lab = lab.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    # plot class examples\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], \n                    y=X[y == cl, 1],\n                    alpha=0.8, \n                    c=colors[idx],\n                    marker=markers[idx], \n                    label=f'Class {cl}', \n                    edgecolor='black')\n\n\nplot_decision_regions(X, y, classifier=ppn)\nplt.xlabel('Sepal length [cm]')\nplt.ylabel('Petal length [cm]')\nplt.legend(loc='upper left')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteReusable code\n\n\n\nThis chunk pulls the Perceptron class from code/ml_utils/perceptron.py. Keeping reusable models in code/ lets Quarto posts stay lean while GitHub readers can browse the source directly."
  },
  {
    "objectID": "posts/daily/2025-11-27/index.html#ml-notes",
    "href": "posts/daily/2025-11-27/index.html#ml-notes",
    "title": "Daily Notes: 2025-11-27",
    "section": "",
    "text": "In which we test whether the perceptron converged and visualize the decision boundaries for this 2-D dataset.\n\nfrom matplotlib.colors import ListedColormap\nfrom ml_utils.perceptron import Perceptron\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ns = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\ndf = pd.read_csv(s, header=None, encoding='utf-8') # dataframe\n\n# print(df.tail()) # just to ensure that the data was loaded correctly\n\n# Goal here is to restrict the task to a binary classification problem\ny = df.iloc[0:100, 4].values # creates a np array (.values) from the species labels (4 == 5th column == species) of the first 100 rows\ny = np.where(y == 'Iris-setosa', 0, 1) # 0 if setosa, 1 if versicolor (first 100 rows only have those two species)\n\n# Goal here is to extract two fairly separable features from the same 100 rows of y\nX = df.iloc[0:100, [0, 2]].values # 100 x 2 matrix that extracts two features: sepal length (0) and petal length (2)\n\n# Plot the two iris classes in the 100 x 2 matrix to visualize how separable they are before fitting a perceptron\n# It's important that it's roughly linearly separable, which is when a perceptron would be a good choice\n\nplt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label = 'Setosa')\nplt.scatter(X[50:100, 0], X[50:100, 1], color='blue', marker='s', label = 'Versicolor')\n\n# Note that the x-axis here would be sepal length (because it's in column 0) and y-axis would be petal length (column 1)\nplt.xlabel('Sepal length (cm)')\nplt.ylabel('Petal length (cm)')\nplt.legend(loc='upper left')\nplt.show()\n\nppn = Perceptron(eta=0.1, n_iter=10)\nppn.fit(X,y) # train Perceptron on the Iris data subset\n\n# Explore how the perceptron's training errors evolve over epochs.\n# range(1, len(ppn.errors_) + 1) just creates the epoch numbers (x-axis)\nplt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o') # draws a point for each training epoch\nplt.xlabel('Epochs')\nplt.ylabel('Number of updates')\nplt.show() # Converges after 6th epoch\n\ndef plot_decision_regions(X, y, classifier, resolution=0.02):\n\n    # setup marker generator and color map\n    markers = ('o', 's', '^', 'v', '&lt;')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    lab = lab.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    # plot class examples\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], \n                    y=X[y == cl, 1],\n                    alpha=0.8, \n                    c=colors[idx],\n                    marker=markers[idx], \n                    label=f'Class {cl}', \n                    edgecolor='black')\n\n\nplot_decision_regions(X, y, classifier=ppn)\nplt.xlabel('Sepal length [cm]')\nplt.ylabel('Petal length [cm]')\nplt.legend(loc='upper left')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteReusable code\n\n\n\nThis chunk pulls the Perceptron class from code/ml_utils/perceptron.py. Keeping reusable models in code/ lets Quarto posts stay lean while GitHub readers can browse the source directly."
  },
  {
    "objectID": "posts/daily/2025-11-27/index.html#personal-notes",
    "href": "posts/daily/2025-11-27/index.html#personal-notes",
    "title": "Daily Notes: 2025-11-27",
    "section": "Personal Notes",
    "text": "Personal Notes\n\nThere are two ways to run Python files:\n\npython script.py runs the file directly.\npython -m package.module runs it as part of a package (which makes package-style imports work).\n\nThink of your project as a bookshelf full of Python packages. When you run python script.py, you’re saying “open this file by its path.” Python doesn’t automatically know where that file sits on the shelf, so package-style imports may fail.\n\nWhen you run python -m ml_utils.plot_decision_regions, you’re saying “look on the shelf for the ml_utils package, take the plot_decision_regions module out, and run it.” Because Python finds the module through the package, it automatically knows about ml_utils and any sibling modules. That’s why relative or absolute imports inside the module work—they’re being run “inside” the package instead of as a standalone file.\n\nA good setup: keep experimental scripts in an experiments/ folder and run them with python -m experiments.some_script from the project root. That way they can import things from ml_utils without extra tricks.\n\n\n\n\n\n\n\nNotePython tips\n\n\n\n\nA module is just a single .py file. For example, perceptron.py or plot_decision_regions.py each define functions/classes and can be imported with their filename (minus .py).\nA package is a folder that contains modules (and usually an __init__.py file). It lets Python treat the folder as a single namespace so you can import things like ml_utils.perceptron.\nThe package provides structure and a shared namespace; the modules are the individual files inside it. When you run python -m ml_utils.plot_decision_regions, you’re telling Python to execute the plot_decision_regions module as part of the ml_utils package, so it knows how to resolve imports like from ml_utils.perceptron import Perceptron."
  },
  {
    "objectID": "posts/daily/2025-11-27/index.html#questions-i-still-have",
    "href": "posts/daily/2025-11-27/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-11-27",
    "section": "Questions I still have",
    "text": "Questions I still have\n\nNeed to resist the urge to brush up on Python basics in a 101 bottom-up style."
  },
  {
    "objectID": "posts/daily/2025-11-27/index.html#tomorrows-plan",
    "href": "posts/daily/2025-11-27/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-11-27",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan\n\nI asked GPT-5.1 Codex to analyze my notes thus far and outline a path for what’s to come:\n\nFinish the perceptron loop: add dataset splits (train/test or cross-validation), compute accuracy/confusion matrices, and document failure cases when classes aren’t separable. This turns the current script into a minimal experiment you can evaluate.\nFeature scaling & preprocessing: implement standardization pipelines to see how learning rate, convergence, and decision boundaries change, especially before moving beyond Iris.\nLogistic regression & soft margins: move from perceptron to logistic regression or linear SVMs to understand probabilistic outputs, loss functions, and regularization—natural evolutions of your current model.\nMulti-class strategies: prototype OvA or OvO wrappers around your perceptron/logistic models to reinforce how binary learners extend to multi-class tasks.\nExperiment tracking: start logging hyperparameters, metrics, and plots (even in your daily notes) so you build the habit of reproducible ML experiments.\nDeepen EDA intuition: answer your own question by comparing outcomes with and without exploratory plots—note when simple visuals caught issues early versus when they weren’t needed."
  },
  {
    "objectID": "posts/daily/2025-11-19/index.html",
    "href": "posts/daily/2025-11-19/index.html",
    "title": "Daily Notes: 2025-11-19",
    "section": "",
    "text": "Training a perceptron model on the Iris dataset\n\nThe perceptron is a binary classifier, so we will consider two flower classes from the Iris dataset for practical reasons. The preceptron algorithm can be extended to multi-class classification using the One vs All (OvA) method (where one class is treated as the positive class and all other classes are the negative class).\nIt’s very important that the feature pair seems roughly linearly separable when judging whether a perceptron is a good choice.\n\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ns = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\ndf = pd.read_csv(s, header=None, encoding='utf-8') # dataframe\n\n# print(df.tail()) # just to ensure that the data was loaded correctly\n\ny = df.iloc[0:100, 4].values # creates a np array (.values) from the species labels (4 == 5th column == species) of the first 100 rows\ny = np.where(y == 'Iris-setosa', 0, 1) # 0 if setosa, 1 if versicolor (first 100 rows only have those two species)\n\nX = df.iloc[0:100, [0, 2]].values # 100 x 2 matrix that extracts two features: sepal length (0) and petal length (2)\n\n# Plot the two iris classes in the 100 x 2 matrix to visualize how separable they are before fitting a perceptron\n# It's important that it's roughly linearly separable, which is when a perceptron would be a good choice\n\nplt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label = 'Setosa')\nplt.scatter(X[50:100, 0], X[50:100, 1], color='blue', marker='s', label = 'Versicolor')\n\n# Note that the x-axis here would be sepal length (because it's in column 0) and y-axis would be petal length (column 1)\nplt.xlabel('Sepal length (cm)')\nplt.ylabel('Petal length (cm)')\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\nSeems like a linear decision boundary is possible, which means that a perceptron can classify this dataset perfectly.\nA perceptron would be an example of a linear classifier"
  },
  {
    "objectID": "posts/daily/2025-11-19/index.html#ml-notes",
    "href": "posts/daily/2025-11-19/index.html#ml-notes",
    "title": "Daily Notes: 2025-11-19",
    "section": "",
    "text": "Training a perceptron model on the Iris dataset\n\nThe perceptron is a binary classifier, so we will consider two flower classes from the Iris dataset for practical reasons. The preceptron algorithm can be extended to multi-class classification using the One vs All (OvA) method (where one class is treated as the positive class and all other classes are the negative class).\nIt’s very important that the feature pair seems roughly linearly separable when judging whether a perceptron is a good choice.\n\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ns = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\ndf = pd.read_csv(s, header=None, encoding='utf-8') # dataframe\n\n# print(df.tail()) # just to ensure that the data was loaded correctly\n\ny = df.iloc[0:100, 4].values # creates a np array (.values) from the species labels (4 == 5th column == species) of the first 100 rows\ny = np.where(y == 'Iris-setosa', 0, 1) # 0 if setosa, 1 if versicolor (first 100 rows only have those two species)\n\nX = df.iloc[0:100, [0, 2]].values # 100 x 2 matrix that extracts two features: sepal length (0) and petal length (2)\n\n# Plot the two iris classes in the 100 x 2 matrix to visualize how separable they are before fitting a perceptron\n# It's important that it's roughly linearly separable, which is when a perceptron would be a good choice\n\nplt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label = 'Setosa')\nplt.scatter(X[50:100, 0], X[50:100, 1], color='blue', marker='s', label = 'Versicolor')\n\n# Note that the x-axis here would be sepal length (because it's in column 0) and y-axis would be petal length (column 1)\nplt.xlabel('Sepal length (cm)')\nplt.ylabel('Petal length (cm)')\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\nSeems like a linear decision boundary is possible, which means that a perceptron can classify this dataset perfectly.\nA perceptron would be an example of a linear classifier"
  },
  {
    "objectID": "posts/daily/2025-11-19/index.html#personal-notes",
    "href": "posts/daily/2025-11-19/index.html#personal-notes",
    "title": "Daily Notes: 2025-11-19",
    "section": "Personal Notes",
    "text": "Personal Notes\n\nI shipped UI improvements to the reader project. Rick Rubin was right - ship improvements to the product by focusing on what I want and need, and hope that there will be others who want the same."
  },
  {
    "objectID": "posts/daily/2025-11-19/index.html#questions-i-still-have",
    "href": "posts/daily/2025-11-19/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-11-19",
    "section": "Questions I still have",
    "text": "Questions I still have\n\nWhen implementing the code, I spent a lot of time trying to figure out why the author wrote each line the way that he did. It made me wonder how much of ML is printing out tails of datasets and plotting data to visualize it as a sanity check."
  },
  {
    "objectID": "posts/daily/2025-11-19/index.html#tomorrows-plan",
    "href": "posts/daily/2025-11-19/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-11-19",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan\n\nFinish implementing the perceptron to classify the Iris dataset."
  },
  {
    "objectID": "posts/daily/2025-11-28/index.html",
    "href": "posts/daily/2025-11-28/index.html",
    "title": "Daily Notes: 2025-11-28",
    "section": "",
    "text": "Adaline: another single-layer neural network\n\nAdaline: Adaptive Linear Neuron. Also known as the Widrow-Hoff rule: Published by Bernard Widrow and Tedd Hoff a few years after Rosenblatt’s perceptron algorithm as an improvement.\nThe Adaline algorithm is important because it introduces the importance of defining and minimizing continuous loss functions.\n\nThis lays the groundwork for other ML classification algorithms such as logistic regression, support vector machines, multilayer neural networks, etc.\n\n\nComparison with Perceptron - Perceptron learning tweaks the weights when the predicted class (the net input and how it compares to the threshold) disagrees with the label. Each update uses eta * (target - predict(x)) and it counts the errors per epoch. - Consider the following loop from Perceptron.fit():\n\nfor _ in range(self.n_iter):\n    errors = 0\n    for xi, target in zip(X, y):\n        update = self.eta * (target - self.predict(xi))\n        self.w_ += update * xi\n        self.b_ += update\n        errors += int(update != 0.0)\n    self.errors_.append(errors)\nreturn self\n\nAdaline uses Gradient Descent to differentiate from Perceptron in 2 important ways:\n\nLinear activation: No more thresholding the net input. Instead, Adaline keeps the raw value net_input = wx + b and compares that to the true continuous target\nCost function and update: Adaline minimizes the sum of squared errors (SSE) between net_input and target.\n\nGradient of SSE w.r.t. weights: (target - net_input) * x\nWeight update for weights & bias: w += eta * (target - net_input) * x\n\n\n\n\n\n\n\n\nNoteWhy is the actual difference important?\n\n\n\nThe gradient using the actual difference is important because the updates become smoother and differentiable. This allows you to apply batch or stochastic gradient descent.\n\n\n\nPseudocode for per-sample gradient descent loop:\n\nCompute net_input for each sample\nCalculate the error which is target - net_input\nDetermine the direction and magnitude to update the weights by multiplying that error by the input vector x and learning rate eta\nAggregate SSE per epoch to monitor whether it is converging\n\n\n\n\n\n\n\n\nNoteWhy is this better than Perceptron?\n\n\n\nAdaline relies on a continuous error surface, which allows it to converge when a Perceptron might oscillate"
  },
  {
    "objectID": "posts/daily/2025-11-28/index.html#ml-notes",
    "href": "posts/daily/2025-11-28/index.html#ml-notes",
    "title": "Daily Notes: 2025-11-28",
    "section": "",
    "text": "Adaline: another single-layer neural network\n\nAdaline: Adaptive Linear Neuron. Also known as the Widrow-Hoff rule: Published by Bernard Widrow and Tedd Hoff a few years after Rosenblatt’s perceptron algorithm as an improvement.\nThe Adaline algorithm is important because it introduces the importance of defining and minimizing continuous loss functions.\n\nThis lays the groundwork for other ML classification algorithms such as logistic regression, support vector machines, multilayer neural networks, etc.\n\n\nComparison with Perceptron - Perceptron learning tweaks the weights when the predicted class (the net input and how it compares to the threshold) disagrees with the label. Each update uses eta * (target - predict(x)) and it counts the errors per epoch. - Consider the following loop from Perceptron.fit():\n\nfor _ in range(self.n_iter):\n    errors = 0\n    for xi, target in zip(X, y):\n        update = self.eta * (target - self.predict(xi))\n        self.w_ += update * xi\n        self.b_ += update\n        errors += int(update != 0.0)\n    self.errors_.append(errors)\nreturn self\n\nAdaline uses Gradient Descent to differentiate from Perceptron in 2 important ways:\n\nLinear activation: No more thresholding the net input. Instead, Adaline keeps the raw value net_input = wx + b and compares that to the true continuous target\nCost function and update: Adaline minimizes the sum of squared errors (SSE) between net_input and target.\n\nGradient of SSE w.r.t. weights: (target - net_input) * x\nWeight update for weights & bias: w += eta * (target - net_input) * x\n\n\n\n\n\n\n\n\nNoteWhy is the actual difference important?\n\n\n\nThe gradient using the actual difference is important because the updates become smoother and differentiable. This allows you to apply batch or stochastic gradient descent.\n\n\n\nPseudocode for per-sample gradient descent loop:\n\nCompute net_input for each sample\nCalculate the error which is target - net_input\nDetermine the direction and magnitude to update the weights by multiplying that error by the input vector x and learning rate eta\nAggregate SSE per epoch to monitor whether it is converging\n\n\n\n\n\n\n\n\nNoteWhy is this better than Perceptron?\n\n\n\nAdaline relies on a continuous error surface, which allows it to converge when a Perceptron might oscillate"
  },
  {
    "objectID": "posts/daily/2025-11-28/index.html#personal-notes",
    "href": "posts/daily/2025-11-28/index.html#personal-notes",
    "title": "Daily Notes: 2025-11-28",
    "section": "Personal Notes",
    "text": "Personal Notes\n\nThis is the first time I’ve seen the mathematical intuition emerge as extremely important.\nIt’s hard to get back into ML studying. I’m trying to keep the big picture in mind of the “why” behind the learning.\nIt’s extremely motivating to listen to how Gabriel Petersson thinks about empowering yourself to learn using AI - getting down to the bottom of things and truly understanding vs. vibecoding."
  },
  {
    "objectID": "posts/daily/2025-11-28/index.html#questions-i-still-have",
    "href": "posts/daily/2025-11-28/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-11-28",
    "section": "Questions I still have",
    "text": "Questions I still have\n\nWhy is Adaline a single layer neural network? I’m assuming it’s because there’s one “decision” that the algorithm makes before it corrects itself per epoch. This is consistent with what I’d expect given the videos I’ve seen of MNNs in action. Would love to understand whether this is correct."
  },
  {
    "objectID": "posts/daily/2025-11-28/index.html#tomorrows-plan",
    "href": "posts/daily/2025-11-28/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-11-28",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan\n\nI need to study up on Gradient Descent to truly understand Adaline. I will watch 3Blue1Brown’s video as well."
  },
  {
    "objectID": "posts/daily/2025-12-13/index.html",
    "href": "posts/daily/2025-12-13/index.html",
    "title": "Daily Notes: 2025-12-13",
    "section": "",
    "text": "Brief Interlude on Python\n\nFor debugging/testing, using python -i as in python3 -i helloworld.py is helpful because it runs the program and then enters the interactive shell afterwards.\n\nThis is helpful when I define a function and then experiment with this defined function\nAlso helpful when I use exception handling\n\nUse snake_case for multiple word variable names instead of camelCase\n\nPrefer lowercase\n\nUse leading _ for “private” or internal names. This is helpful because it reduces accidental use outside a module/class. Not real access control.\n\nE.g., def _internal_helper\nfrom module import * will skip underscored names\n\nYou can add placeholders for statements to be added later with a pass such as below:\n\n\nif name in namelist:\n    # not evaluated yet\n    pass\nelse:\n    # statements\n\n\nYou have to close files after opening and using them, like as follows:\n\n\nf = open('foo.txt','r')\n# Use f\nf.close()\n\n\nYou can achieve the same purpose using with and it automatically closes:\n\n\nwith open('foo.txt', 'r') as f:\n    # automatically closes afterwards\n\n    # for line-by-line reads\n    for line in f:\n        # Process the line\n    \n    # for reading the entire file into a strong\n    data = f.read()\n\n    # for writing into a file\n    f.write('some text\\n')\n\n\n\n\nException Handling: This is how Python deals with things going wrong. Errors are “exceptions”\nWrap risky code in try blocks\nWith proper exception handling, if an error/exception occurs, control jumps to an except block.\nWithout an except block, any runtime error can crash our script.\nWith an except block:\n\nLong-running programs stay alive after isolated failures\nFriendly error messages instead of tracebacks\nDifferentiate between expected issues (aka user error) and bugs\n\nImportantly, the name must match the type of error you’re trying to catch (e.g., ValueError, ZeroDivisionError)\nExceptions have associated values that can be passed to variables (e.g., except ValueError as e)\nAlso, finally can be used for cleanup and for code that must run whether or not an exception occurs\n\nAlso useful for managing resources (esp. files)\n\n\n\ntry:\n    result = divide(num, denom)\nexcept ZeroDivisionError:\n    print(\"Can't divide by zero—please try again.\")\nexcept ValueError as e: # can catch different kinds of exceptions, and prints the associated value\n    print(\"Value error—please try again.\", e)\nelse:\n    print(\"Result:\", result)\nfinally:\n    cleanup_resources()\n\n\nHere’s something I wrote, which also includes an f-string that allows me to drop values into the string with {…}\n\n\ndef portfolio_cost(file_name):\n    total_cost = 0.0\n\n    with open(file_name, 'r') as f:\n        for line in f:\n            fields = line.split()\n            try:\n                number_shares = float(fields[1])\n                purchase_price = float(fields[2])\n                total_cost += number_shares * purchase_price\n            except ValueError as e:\n                print(f\"Couldn't parse: {line.strip()}\")\n                print(\"Reason: \", e)\n    return total_cost\n\n\n\n\n\nAll of the basic data types (strings, integers, lists, etc.) are Objects, therefore they involve “methods” to carry out operations\nYou make your own Objects as follows (with their own methods):\n\n\nclass Player:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n        self.health = 100\n    \n    def move(self, points):\n        self.health -= points\n\n\n__init__ method initializes a new instance and is important for storing data attributes\nBy convention, the method is called on self and self is defined as the first argument\n\nThe actual name is unimportant. Just know that the object is always passed as the first argument.\nJust conventional Python style to call it self\n\nimport module as m imports a module (a namespace) that are the (global) variables and functions defined in module.py\n\nEverything defined with global scope can be called as m.method()\nfrom module import function allows you to import certain functions from a module and put them into local scope. No need to use the module prefix module.function()\nfrom module import * allows you to import all functions. Don’t use this often in practice because it impacts readability.\n\n\n\n\n\n\nThree options for data structures:\n\ntuples\ndictionaries\nclasses (user-defined)\n\nTuples are a collection of values (e.g., s = (GOOG, 10, 300.20)).\n\nCan be used like an array (quantity = s[1])\nCan be unpacked into separate variables (name, quantity, price = s).\nImmutable (s[1] = 1 throws a TypeError)\n\nDictionaries are an unordered collection of key-value pairs (e.g., s = {‘name’: GOOG, …})\n\nUse the key values to access (google_price = s[price])\nCan be modified (s[price] = 320.20)\n\nUser-Defined Classes as defined above provide the nice clean syntax we know and love (e.g., s.name).\nSome “advanced” variants that are worth knowing:\n\nSlots: Saves memory\nDataclasses: Reduces coding\nNamed Tuples: Immutability/Tuple behavior\n\nWhy are Slots so great?\n\n__slots__ are useful as a performance optimization because Python typically creates a __dict__ for the attributes of every instance of an object.\nThese per-instance dictionaries allow you to add arbitary attributes at runtime, but have high memory overhead.\nIn exchange for enumerating the attributes your instances will ever need in advance, a fixed slot layout saves the memory overhead of per-instance dictionaries.\nThis is especially useful when you have many objects.\n\n\n\nclass Player:\n    __slots__('name', 'age', 'health')\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n        self.health = 100\n\n\nWhy are Dataclasses so great?\n\nIt’s just easier… it helps make boilerplate-free classes.\nPython auto-generates helpers like __init__\nYou do need to annotate the types (even if it’s just Any)\n\n\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass Player:\n    name: str\n    age: int\n    health: int\n\n\nWhy are Named Tuples so great?\n\nDataclasses with guaranteed immutability.\nThe most important feature is Immutability, which allows you to safely use them as dict keys\nOther features for tuples (indexing, etc.) are useful here too.\n\n\n\nimport typing\n\nclass Player(typing.NamedTuple):\n    name: str\n    age: int\n    health: int"
  },
  {
    "objectID": "posts/daily/2025-12-13/index.html#ml-notes",
    "href": "posts/daily/2025-12-13/index.html#ml-notes",
    "title": "Daily Notes: 2025-12-13",
    "section": "",
    "text": "Brief Interlude on Python\n\nFor debugging/testing, using python -i as in python3 -i helloworld.py is helpful because it runs the program and then enters the interactive shell afterwards.\n\nThis is helpful when I define a function and then experiment with this defined function\nAlso helpful when I use exception handling\n\nUse snake_case for multiple word variable names instead of camelCase\n\nPrefer lowercase\n\nUse leading _ for “private” or internal names. This is helpful because it reduces accidental use outside a module/class. Not real access control.\n\nE.g., def _internal_helper\nfrom module import * will skip underscored names\n\nYou can add placeholders for statements to be added later with a pass such as below:\n\n\nif name in namelist:\n    # not evaluated yet\n    pass\nelse:\n    # statements\n\n\nYou have to close files after opening and using them, like as follows:\n\n\nf = open('foo.txt','r')\n# Use f\nf.close()\n\n\nYou can achieve the same purpose using with and it automatically closes:\n\n\nwith open('foo.txt', 'r') as f:\n    # automatically closes afterwards\n\n    # for line-by-line reads\n    for line in f:\n        # Process the line\n    \n    # for reading the entire file into a strong\n    data = f.read()\n\n    # for writing into a file\n    f.write('some text\\n')\n\n\n\n\nException Handling: This is how Python deals with things going wrong. Errors are “exceptions”\nWrap risky code in try blocks\nWith proper exception handling, if an error/exception occurs, control jumps to an except block.\nWithout an except block, any runtime error can crash our script.\nWith an except block:\n\nLong-running programs stay alive after isolated failures\nFriendly error messages instead of tracebacks\nDifferentiate between expected issues (aka user error) and bugs\n\nImportantly, the name must match the type of error you’re trying to catch (e.g., ValueError, ZeroDivisionError)\nExceptions have associated values that can be passed to variables (e.g., except ValueError as e)\nAlso, finally can be used for cleanup and for code that must run whether or not an exception occurs\n\nAlso useful for managing resources (esp. files)\n\n\n\ntry:\n    result = divide(num, denom)\nexcept ZeroDivisionError:\n    print(\"Can't divide by zero—please try again.\")\nexcept ValueError as e: # can catch different kinds of exceptions, and prints the associated value\n    print(\"Value error—please try again.\", e)\nelse:\n    print(\"Result:\", result)\nfinally:\n    cleanup_resources()\n\n\nHere’s something I wrote, which also includes an f-string that allows me to drop values into the string with {…}\n\n\ndef portfolio_cost(file_name):\n    total_cost = 0.0\n\n    with open(file_name, 'r') as f:\n        for line in f:\n            fields = line.split()\n            try:\n                number_shares = float(fields[1])\n                purchase_price = float(fields[2])\n                total_cost += number_shares * purchase_price\n            except ValueError as e:\n                print(f\"Couldn't parse: {line.strip()}\")\n                print(\"Reason: \", e)\n    return total_cost\n\n\n\n\n\nAll of the basic data types (strings, integers, lists, etc.) are Objects, therefore they involve “methods” to carry out operations\nYou make your own Objects as follows (with their own methods):\n\n\nclass Player:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n        self.health = 100\n    \n    def move(self, points):\n        self.health -= points\n\n\n__init__ method initializes a new instance and is important for storing data attributes\nBy convention, the method is called on self and self is defined as the first argument\n\nThe actual name is unimportant. Just know that the object is always passed as the first argument.\nJust conventional Python style to call it self\n\nimport module as m imports a module (a namespace) that are the (global) variables and functions defined in module.py\n\nEverything defined with global scope can be called as m.method()\nfrom module import function allows you to import certain functions from a module and put them into local scope. No need to use the module prefix module.function()\nfrom module import * allows you to import all functions. Don’t use this often in practice because it impacts readability.\n\n\n\n\n\n\nThree options for data structures:\n\ntuples\ndictionaries\nclasses (user-defined)\n\nTuples are a collection of values (e.g., s = (GOOG, 10, 300.20)).\n\nCan be used like an array (quantity = s[1])\nCan be unpacked into separate variables (name, quantity, price = s).\nImmutable (s[1] = 1 throws a TypeError)\n\nDictionaries are an unordered collection of key-value pairs (e.g., s = {‘name’: GOOG, …})\n\nUse the key values to access (google_price = s[price])\nCan be modified (s[price] = 320.20)\n\nUser-Defined Classes as defined above provide the nice clean syntax we know and love (e.g., s.name).\nSome “advanced” variants that are worth knowing:\n\nSlots: Saves memory\nDataclasses: Reduces coding\nNamed Tuples: Immutability/Tuple behavior\n\nWhy are Slots so great?\n\n__slots__ are useful as a performance optimization because Python typically creates a __dict__ for the attributes of every instance of an object.\nThese per-instance dictionaries allow you to add arbitary attributes at runtime, but have high memory overhead.\nIn exchange for enumerating the attributes your instances will ever need in advance, a fixed slot layout saves the memory overhead of per-instance dictionaries.\nThis is especially useful when you have many objects.\n\n\n\nclass Player:\n    __slots__('name', 'age', 'health')\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n        self.health = 100\n\n\nWhy are Dataclasses so great?\n\nIt’s just easier… it helps make boilerplate-free classes.\nPython auto-generates helpers like __init__\nYou do need to annotate the types (even if it’s just Any)\n\n\n\nfrom dataclasses import dataclass\n\n@dataclass\nclass Player:\n    name: str\n    age: int\n    health: int\n\n\nWhy are Named Tuples so great?\n\nDataclasses with guaranteed immutability.\nThe most important feature is Immutability, which allows you to safely use them as dict keys\nOther features for tuples (indexing, etc.) are useful here too.\n\n\n\nimport typing\n\nclass Player(typing.NamedTuple):\n    name: str\n    age: int\n    health: int"
  },
  {
    "objectID": "posts/daily/2025-12-13/index.html#personal-notes",
    "href": "posts/daily/2025-12-13/index.html#personal-notes",
    "title": "Daily Notes: 2025-12-13",
    "section": "Personal Notes",
    "text": "Personal Notes\n\nI’m feeling a lot of anxiety to start “doing” rather than just consuming information, step by step, from a textbook. I’ve been coding more aggressively as a result.\nA reminder to myself on how learning via a textbook should feel:\n\nlike pieces of a puzzle retroactively coming together\nlike previous experiences making far more sense in context\nlike seeing in the light what I once grasped around for in the dark\n\nI’m trying to avoid tutorial hell by building until I get stuck, and grasping around for the necessary context as I go."
  },
  {
    "objectID": "posts/daily/2025-12-13/index.html#questions-i-still-have",
    "href": "posts/daily/2025-12-13/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-12-13",
    "section": "Questions I still have",
    "text": "Questions I still have"
  },
  {
    "objectID": "posts/daily/2025-12-13/index.html#tomorrows-plan",
    "href": "posts/daily/2025-12-13/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-12-13",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan"
  },
  {
    "objectID": "posts/projects/index.html",
    "href": "posts/projects/index.html",
    "title": "Projects & Experiments",
    "section": "",
    "text": "Small experiments and projects: training models, reproducing papers, and playing with datasets.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/daily/2025-12-14/index.html",
    "href": "posts/daily/2025-12-14/index.html",
    "title": "Daily Notes: 2025-12-14",
    "section": "",
    "text": "Project-Based Learning (Sabbatical Day 1)\n\n\n\nBefore any task, it’s important to prepare the data by understanding the columns, understand the target definition, and define safe and consistent train/val/test splits.\nQuestions you should be asking: What columns are categorical vs. numeric? What are the names of any important columns? Are there any missing values that need imputation?\nAlso great to run df.head() and df.info() and histograms/bar charts.\nIn our case, df['Churn'].value_counts() is great to understand the raw counts of each label. How many churners are there? How rare is it?\n\nThis can also help stratify your train/val/test splits so each set preserves the ratio (no accidental “easy” test set).\n\nWe’re using a helper function for load_data. Why? It’s important to be consistent about preprocessing.\n\nStratifying into train/val/test is consistent for each experiment.\nYou can document the random seed, i.e., the number provided to a random number generator so that the rows are shuffled the same way.\n\nThe goal is to be consistent about shuffling and splitting the data!\n\n\n\"\"\"Data loading utilities: fetch OpenML dataset and split into train/val/test.\"\"\"\nfrom typing import Tuple\nfrom sklearn.datasets import fetch_openml\n\ndef load_data(dataset_id: str, test_size: float = 0.2, val_size: float = 0.1, random_state: int = 42) -&gt; Tuple[object, object, object, object]:\n    \"\"\"Fetch dataset via OpenML, then return (X_train, X_val, X_test, y variants).\n\n    Placeholder: use sklearn.datasets.fetch_openml, then train/val/test split.\n    Over-explain steps when implemented so future readers learn why each choice matters.\n    \"\"\"\n    # Download the raw table from OpenML to understand the dataset\n    churn_data = fetch_openml(data_id=\"45568\")\n\n\n\n\n\nPutting reusable logic code under src/ (short for “source”) is conventional for keeping the project tidy\nThe reusable functions, models, training data is all within src/ and reports, implementation, etc. are separate.\nPython also treats src/ as the source root, so relative imports stay clean."
  },
  {
    "objectID": "posts/daily/2025-12-14/index.html#ml-notes",
    "href": "posts/daily/2025-12-14/index.html#ml-notes",
    "title": "Daily Notes: 2025-12-14",
    "section": "",
    "text": "Project-Based Learning (Sabbatical Day 1)\n\n\n\nBefore any task, it’s important to prepare the data by understanding the columns, understand the target definition, and define safe and consistent train/val/test splits.\nQuestions you should be asking: What columns are categorical vs. numeric? What are the names of any important columns? Are there any missing values that need imputation?\nAlso great to run df.head() and df.info() and histograms/bar charts.\nIn our case, df['Churn'].value_counts() is great to understand the raw counts of each label. How many churners are there? How rare is it?\n\nThis can also help stratify your train/val/test splits so each set preserves the ratio (no accidental “easy” test set).\n\nWe’re using a helper function for load_data. Why? It’s important to be consistent about preprocessing.\n\nStratifying into train/val/test is consistent for each experiment.\nYou can document the random seed, i.e., the number provided to a random number generator so that the rows are shuffled the same way.\n\nThe goal is to be consistent about shuffling and splitting the data!\n\n\n\"\"\"Data loading utilities: fetch OpenML dataset and split into train/val/test.\"\"\"\nfrom typing import Tuple\nfrom sklearn.datasets import fetch_openml\n\ndef load_data(dataset_id: str, test_size: float = 0.2, val_size: float = 0.1, random_state: int = 42) -&gt; Tuple[object, object, object, object]:\n    \"\"\"Fetch dataset via OpenML, then return (X_train, X_val, X_test, y variants).\n\n    Placeholder: use sklearn.datasets.fetch_openml, then train/val/test split.\n    Over-explain steps when implemented so future readers learn why each choice matters.\n    \"\"\"\n    # Download the raw table from OpenML to understand the dataset\n    churn_data = fetch_openml(data_id=\"45568\")\n\n\n\n\n\nPutting reusable logic code under src/ (short for “source”) is conventional for keeping the project tidy\nThe reusable functions, models, training data is all within src/ and reports, implementation, etc. are separate.\nPython also treats src/ as the source root, so relative imports stay clean."
  },
  {
    "objectID": "posts/daily/2025-12-14/index.html#personal-notes",
    "href": "posts/daily/2025-12-14/index.html#personal-notes",
    "title": "Daily Notes: 2025-12-14",
    "section": "Personal Notes",
    "text": "Personal Notes\n\nToday is Day 1 of my one week sabbatical, a sprint to cover as much ML mastery as possible by the end of the week.\nTo approach this from a project-based learning perspective, I chose to create the following project\nNeed to understand trees, kNN, ensembles, SVMs/kernels, theory/VC, randomized optimization, information theory, Bayesian learning, clustering/EM, ICA/manifold learning, RL.\nTherefore, I chose a project that is not just following along with Andrej Karpathy’s “Zero to Hero” and more of a “classical” ML problem.\n\nI will do an LLM-focused project as a 2-day “attention from scratch” mini-lab at the end.\nLLM-focused project: Great for creating a deep intuition for gradients, cross-entropy, batching, etc.\n\nI’ll also get in the habit of writing reports. For each experiment, write a 2–3 sentence “hypothesis → result → takeaway”.\n\n\nProject Goals\n\nCompare multiple supervised algorithms: decision trees, kNN, ensembles, SVMs/kernels, neural nets.\nDo optimization & uncertainty in a practical way: randomized search / hyperparameter tuning, thinking about inductive bias + generalization, and even “deconstructing AdamW” concepts via controlled experiments.\nDo unsupervised: clustering + EM/GMM intuition, feature selection/transformation (ICA, manifold learning).\nUse the exact tooling expectations: sklearn pipelines/CV/calibration + PyTorch MLP.\n\n\n\nProject scope\n\nI chose a tabular churn analysis task: using ML models to predict customer churn using structured datasets.\nUsing the telco-customer-churn dataset.\n\n\nDeliverables\n\nsklearn baselines (fast, high learning ROI): Run and compare:\n\n\nLogistic Regression (strong baseline)\nDecision Tree\nRandom Forest / Gradient Boosting\nSVM (RBF or linear)\nkNN All using the same Pipeline(preprocess → model) and Stratified CV.\n\nMetrics to report\n\nROC-AUC + PR-AUC (PR-AUC is important if churn is imbalanced)\nF1 (macro or positive-class)\nConfusion matrix\nCalibration curve (optional but great)\n\n\nPyTorch model (beginner-friendly): Train a small MLP for churn:\n\n\nOption A (simplest): reuse the sklearn preprocessing output (one-hot + scaled) and feed that into an MLP.\nOption B (more “deep learning”): learn embeddings for categorical columns + concat with scaled numeric features. Either way, you’ll learn the “real” deep learning loop: batching, loss, optimizer, early stopping.\n\n\n\n\nSummary: Scope and Learning Plan\n\nDataset: Telco Customer Churn (openml id=45568), tabular, binary target.\nPrimary goal: Learn ML by comparing diverse models, not just finishing quickly.\nBaselines (sklearn, all via Pipeline + Stratified CV): Logistic Regression, Decision Tree, Random Forest/Gradient Boosting, SVM (RBF or linear), kNN. Report ROC-AUC, PR-AUC, F1 (positive class), confusion matrix, and optionally calibration curves.\nHyperparameter tuning: Small, educational RandomizedSearchCV grids to see variance and inductive bias effects. Log mean/std CV scores and settings.\nPyTorch MLP: Start with preprocessed tabular features (one-hot + scaled). Train a small MLP with batching, optimizer choice (SGD vs Adam/AdamW), early stopping. Later, try embeddings for categorical features.\nUnsupervised explorations: Clustering (k-means, GMM/EM) to inspect churn by cluster; dimensionality reductions (PCA/ICA; t-SNE/UMAP for visualization); feature importance/selection (tree/permutation-based).\nReporting habit: For each experiment, write a short entry in report/report.md (hypothesis → setup → metrics → takeaway) and save plots to report/figures/ (ROC/PR curves, confusion matrices, calibration, learning curves).\nData discipline: Use consistent train/val/test splits (stratified), avoid leakage, and keep preprocessing inside the pipeline so CV/test use the exact same transforms."
  },
  {
    "objectID": "posts/daily/2025-12-08/index.html",
    "href": "posts/daily/2025-12-08/index.html",
    "title": "Daily Notes: 2025-12-08",
    "section": "",
    "text": "Gradient Descent + Returning to Adaline\n\n\n\nFind the derivative of the Loss Function for each parameter in it. (i.e., take the Gradient of the Loss Function).\nPick random values for the parameters, and plug them into the Gradient.\nCalculate the step sizes, where step size = learning rate * Gradient\nCalculate the new parameters, where new parameter = old parameter - step size (the minus sign moves you downhill)\nRepeat 3 - 4 until the step size is very small or you reach a max number of steps.\n\n\nFor a single parameter, the Gradient is the slope of the loss with respect to that parameter (slope == Gradient).\nWhen you do this over multiple weights, Gradient Descent also helps you understand which weights and connections matter more in a given step\nGradient Descent encodes the relative importance of each weight\nTherefore, Gradient Descent is learning defined by minimizing the output of a loss function.\n\n\n\n\nReminder from 11/28 Notes:\n\nAdaline uses Gradient Descent to differentiate from Perceptron in 2 important ways:\n\nLinear activation: No more thresholding the net input. Instead, Adaline keeps the raw value net_input = wx + b and compares that to the true continuous target\nCost function and update: Adaline minimizes the sum of squared errors (SSE) between net_input and target.\n\nGradient of SSE w.r.t. weights: (target - net_input) * x\nWeight update for weights & bias: w += eta * (target - net_input) * x\n\n\n\n\n\n\n\n\n\nNoteWhy is the actual difference important?\n\n\n\nThe gradient using the actual difference is important because the updates become smoother and differentiable. This allows you to apply batch or stochastic gradient descent.\n\n\n\nPseudocode for per-sample gradient descent loop:\n\nCompute net_input for each sample\nCalculate the error which is target - net_input\nDetermine the direction and magnitude to update the weights by multiplying that error by the input vector x and learning rate eta\nAggregate SSE per epoch to monitor whether it is converging\n\n\n\n\n\n\n\n\nNoteWhy is this better than Perceptron?\n\n\n\nAdaline relies on a continuous error surface, which allows it to converge when a Perceptron might oscillate\n\n\nImplementation (annotation comes tomorrow)\n\nfrom __future__ import annotations\nfrom mimetypes import init\n\nimport numpy as np\n\n__all__ = [\"AdalineGD\"]\n\nclass AdalineGD:\n    \"\"\"ADAptive LInear NEuron classifier.\n\n    Parameters\n    --------------\n\n    eta: float. \n        This is the learning rate (between 0.0 and 1.0)\n    n_iter: int\n        Passes over the training dataset\n    random_state: int\n        Random number generator seed for random weight generalization\n    \n    Attributes\n    ---------------\n    w_: 1D-Array\n        Weights after fitting\n    b_: Scalar\n        Bias after fitting\n    losses_: list\n        Mean Squared Error loss function values in each epoch\n    \"\"\"\n    def __init__(self, eta=0.01, n_iter = 50, random_state = 1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def net_input(self, X):\n        \"\"\"Calculate net input\"\"\"\n        return np.dot(X, self.w_) + self.b_\n\n    def activation(self, X):\n        \"\"\"Compute linear activation\"\"\"\n        return X\n    \n    def predict(self, X): \n        \"\"\"Return class label after unit step\"\"\"\n        return np.where(self.activation(self.net_input(X)) &gt;= 0.5, 1, 0)\n\n    def fit(self, X, y):\n        \"\"\" Fit training data.\n\n        Parameters\n        -----------\n        X: array-like, shape = [m_examples, n_features]\n            Training vectors\n            m_examples: # of examples\n            n_features: # of features\n        \n        y: array-like, shape = [m_examples]\n            Target values\n        \n        Returns\n        -----------\n        self: object\n        \"\"\"\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n        self.b_ = np.float_(0.)\n        self.losses_ = []\n\n        for i in range(self.n_iter): # for each epoch\n            net_input = self.net_input(X) \n            output = self.activation(net_input)\n            errors = (y - output)\n            \"\"\"This step is important!\n            We are calculating the gradient based on the whole training set,\n            not just evaluating each individual training example (as in the perceptron).\n            \"\"\"\n            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]\n            self.b_ += self.eta * 2.0 * errors.mean() \n            loss = (errors**2).mean()\n            self.losses_.append(loss)\n        return self"
  },
  {
    "objectID": "posts/daily/2025-12-08/index.html#ml-notes",
    "href": "posts/daily/2025-12-08/index.html#ml-notes",
    "title": "Daily Notes: 2025-12-08",
    "section": "",
    "text": "Gradient Descent + Returning to Adaline\n\n\n\nFind the derivative of the Loss Function for each parameter in it. (i.e., take the Gradient of the Loss Function).\nPick random values for the parameters, and plug them into the Gradient.\nCalculate the step sizes, where step size = learning rate * Gradient\nCalculate the new parameters, where new parameter = old parameter - step size (the minus sign moves you downhill)\nRepeat 3 - 4 until the step size is very small or you reach a max number of steps.\n\n\nFor a single parameter, the Gradient is the slope of the loss with respect to that parameter (slope == Gradient).\nWhen you do this over multiple weights, Gradient Descent also helps you understand which weights and connections matter more in a given step\nGradient Descent encodes the relative importance of each weight\nTherefore, Gradient Descent is learning defined by minimizing the output of a loss function.\n\n\n\n\nReminder from 11/28 Notes:\n\nAdaline uses Gradient Descent to differentiate from Perceptron in 2 important ways:\n\nLinear activation: No more thresholding the net input. Instead, Adaline keeps the raw value net_input = wx + b and compares that to the true continuous target\nCost function and update: Adaline minimizes the sum of squared errors (SSE) between net_input and target.\n\nGradient of SSE w.r.t. weights: (target - net_input) * x\nWeight update for weights & bias: w += eta * (target - net_input) * x\n\n\n\n\n\n\n\n\n\nNoteWhy is the actual difference important?\n\n\n\nThe gradient using the actual difference is important because the updates become smoother and differentiable. This allows you to apply batch or stochastic gradient descent.\n\n\n\nPseudocode for per-sample gradient descent loop:\n\nCompute net_input for each sample\nCalculate the error which is target - net_input\nDetermine the direction and magnitude to update the weights by multiplying that error by the input vector x and learning rate eta\nAggregate SSE per epoch to monitor whether it is converging\n\n\n\n\n\n\n\n\nNoteWhy is this better than Perceptron?\n\n\n\nAdaline relies on a continuous error surface, which allows it to converge when a Perceptron might oscillate\n\n\nImplementation (annotation comes tomorrow)\n\nfrom __future__ import annotations\nfrom mimetypes import init\n\nimport numpy as np\n\n__all__ = [\"AdalineGD\"]\n\nclass AdalineGD:\n    \"\"\"ADAptive LInear NEuron classifier.\n\n    Parameters\n    --------------\n\n    eta: float. \n        This is the learning rate (between 0.0 and 1.0)\n    n_iter: int\n        Passes over the training dataset\n    random_state: int\n        Random number generator seed for random weight generalization\n    \n    Attributes\n    ---------------\n    w_: 1D-Array\n        Weights after fitting\n    b_: Scalar\n        Bias after fitting\n    losses_: list\n        Mean Squared Error loss function values in each epoch\n    \"\"\"\n    def __init__(self, eta=0.01, n_iter = 50, random_state = 1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def net_input(self, X):\n        \"\"\"Calculate net input\"\"\"\n        return np.dot(X, self.w_) + self.b_\n\n    def activation(self, X):\n        \"\"\"Compute linear activation\"\"\"\n        return X\n    \n    def predict(self, X): \n        \"\"\"Return class label after unit step\"\"\"\n        return np.where(self.activation(self.net_input(X)) &gt;= 0.5, 1, 0)\n\n    def fit(self, X, y):\n        \"\"\" Fit training data.\n\n        Parameters\n        -----------\n        X: array-like, shape = [m_examples, n_features]\n            Training vectors\n            m_examples: # of examples\n            n_features: # of features\n        \n        y: array-like, shape = [m_examples]\n            Target values\n        \n        Returns\n        -----------\n        self: object\n        \"\"\"\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n        self.b_ = np.float_(0.)\n        self.losses_ = []\n\n        for i in range(self.n_iter): # for each epoch\n            net_input = self.net_input(X) \n            output = self.activation(net_input)\n            errors = (y - output)\n            \"\"\"This step is important!\n            We are calculating the gradient based on the whole training set,\n            not just evaluating each individual training example (as in the perceptron).\n            \"\"\"\n            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]\n            self.b_ += self.eta * 2.0 * errors.mean() \n            loss = (errors**2).mean()\n            self.losses_.append(loss)\n        return self"
  },
  {
    "objectID": "posts/daily/2025-12-08/index.html#personal-notes",
    "href": "posts/daily/2025-12-08/index.html#personal-notes",
    "title": "Daily Notes: 2025-12-08",
    "section": "Personal Notes",
    "text": "Personal Notes\n\n3Blue1Brown brought up an interesting point in his Neural Networks intro video that representation in terms of linear algebra is useful because of the libraries that have already been built and optimized for fast performance (numpy). Might seem obvious, but cool.\n3Blue1Brown also mentioned that Gradient Descent is a good reason why the neural network calculations (the neuron calculations involving weights & biases) should be continuous and not discrete values.\nRemember that the neural network was never told what patterns to look for… Gradient Descent can often figure it out. It’s crazy how much you can do just by minimizing loss across the training data."
  },
  {
    "objectID": "posts/daily/2025-12-08/index.html#questions-i-still-have",
    "href": "posts/daily/2025-12-08/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-12-08",
    "section": "Questions I still have",
    "text": "Questions I still have\n\nIt’s not quite clear how the errors array contains partial derivatives?\nI am still not 100% confident with my file structure (where I save my .py implementations and how I run it within my Quarto notes)"
  },
  {
    "objectID": "posts/daily/2025-12-08/index.html#tomorrows-plan",
    "href": "posts/daily/2025-12-08/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-12-08",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan\n\nGo through the Adaline implementation with GPT-5.1 Codex, line by line, until I can fully explain what each step is doing."
  },
  {
    "objectID": "posts/daily/2025-11-18/index.html",
    "href": "posts/daily/2025-11-18/index.html",
    "title": "Daily Notes: 2025-11-18",
    "section": "",
    "text": "Explorations using open-source local models."
  },
  {
    "objectID": "posts/daily/2025-11-18/index.html#ml-notes",
    "href": "posts/daily/2025-11-18/index.html#ml-notes",
    "title": "Daily Notes: 2025-11-18",
    "section": "",
    "text": "Explorations using open-source local models."
  },
  {
    "objectID": "posts/daily/2025-11-18/index.html#personal-notes",
    "href": "posts/daily/2025-11-18/index.html#personal-notes",
    "title": "Daily Notes: 2025-11-18",
    "section": "Personal Notes",
    "text": "Personal Notes\n\nI forked Karpathy’s reader3 and created a version with a built-in local LLM: reader. I wrote up the value proposition in a reply tweet, and I couldn’t believe my eyes when I received a like from Karpathy himself. It was incredibly motivating and I spent the rest of the day shipping improvements.\nI learned a lot about Ollama, Llama 3.2, and about open-source models that can be run locally and “on the edge.” I’d love to experiment with more of the advanced SOTA open-source models, but I ran against the limits of my 2020 x64 16GB MacBook. I think the amazing feat of lightweight open-source models is something that isn’t discussed enough. It’s sad that this appears to be taking the contours of a political statement given the China vs. US AI arms race and the relative positions both players have undertaken.\nI finally redownloaded Cursor. I was an “early adopter” of Cursor in 2023, and I was an active “vibe coder” in 2024-2025. I stopped “vibe coding” on Replit in April of 2025, and 6 months later, using GPT-5.1 Codex, it is incredible how much better it has become. “This is the worst that AI will ever be” is a statement with wide-ranging implications."
  },
  {
    "objectID": "posts/daily/2025-11-18/index.html#questions-i-still-have",
    "href": "posts/daily/2025-11-18/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-11-18",
    "section": "Questions I still have",
    "text": "Questions I still have\n\nI still don’t think I fully understand the possibilities that can be unlocked by locally run open-source models. What are some workflows that I have today that are easier, and faster, if I run them locally?"
  },
  {
    "objectID": "posts/daily/2025-11-18/index.html#tomorrows-plan",
    "href": "posts/daily/2025-11-18/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-11-18",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan\n\nWould love to get back to pure ML studying tomorrow.\nWould like to read Raschka’s Converting Llama 2 to Llama 3.2 From Scratch."
  },
  {
    "objectID": "posts/daily/2025-11-29/index.html",
    "href": "posts/daily/2025-11-29/index.html",
    "title": "Daily Notes: 2025-11-29",
    "section": "",
    "text": "Gradient Descent, or how machines learn\n\nWhen measuring how “costly” a wrong prediction is / how “accurate” a training example was, you can use a loss function.\nOne example of a loss function is the Sum of Squared Errors (SSE). Formally defined as:\n\n\\(SSE = \\sum\\limits_{i} (y_i - \\hat{y}_i)^2\\)\n\n\n\n\n\n\nNoteWhat’s great about SSE?\n\n\n\n\nIt is said to be appropriate when modeling regression with Gaussian noise.\nIt is not appropriate for classification or distributions with outliers (perhaps because it penalizes large errors heavily?).\n\n\n\n\nRecall that Weights \\(w\\) represent how strongly each input dimension influences the neuron, and the Bias \\(b\\) shifts the activation threshold and acts as an offset.\n\n3Blue1Brown says: Neurons are connected to the neurons in the previous layer. The weights are the strength of those connections, and for ReLU-like activations, the bias affects when the neuron is active/inactive.\nReLU here refers to a Rectified Linear Unit, and it can be defined as \\(ReLU(a) = max(0, a)\\). Basically, the sigmoid was a slow learner and hard to train, and the ReLU made things easier by making a neuron “active” (\\(a\\)) or “inactive” (\\(0\\)).\n\nA network “learns” by adjusting the parameters \\(W\\) and \\(b\\) to minimize a cost function / loss function.\n\n\n\n\n\n\n\nNoteW vs. w\n\n\n\n\\(W\\) and \\(w\\) mean different things in neural networks.\n\\(W\\): A weight matrix for an entire layer. If a layer has \\(n_{in}\\) inputs and \\(m_{out}\\) neurons, then \\(W \\in R^{n_{in} \\times m_{out}}\\). Each row is the weight vector of one neuron, and \\(z = Wx + b\\) is the equivalent of doing many individual dot products at once.\n\\(w\\): A weight vector for a single neuron.\n\\(w = \\begin{bmatrix}\nw_1 \\\\\nw_2 \\\\\n\\vdots \\\\\nw_n\n\\end{bmatrix}\\)\n\n\nFormally, Gradient Descent is the algorithm used to minimize a loss function \\(J(\\theta)\\) by iteratively updating parameters in the direction that reduces the loss:\n\\(\\theta := \\theta - \\eta \\nabla_{\\theta} J(\\theta)\\)\n\nWhat is great about Gradient Descent is that the network can minimize this loss function by converging towards a local minimum, even when the derivative of the minimum is not 0.\nGradient Descent, by design, can take big “steps” towards the minimum when it determines that it is far away, and smaller steps when it determines that it is closer.\nThink of a “step” as the derivative of the sum of squared residuals with respect to a certain parameter, multiplied by a learning rate\nGradient Descent stops taking steps when the “step size” is very close to 0 or if it is forced to give up (i.e., it surpasses the maximum number of steps)\n\n\n\n\n\n\n\nNoteWhy is it called Gradient Descent?\n\n\n\n\nA Gradient is two or more derivatives of the same function\nA Gradient gives the direction of steepest increase (from Multivariable Calculus)\nWe use this Gradient to descend in the direction of \\(-\\nabla J(\\theta)\\) to reach the lowest point of the loss function.\n\n\n\nGradient Descent in plain English:\n\nFind the derivative of the Loss Function for each parameter in it. (i.e., take the Gradient of the Loss Function).\nPick random values for the parameters, and plug them into the Gradient.\nCalculate the step sizes, where step size == slope * learning rate\nCalculate the new parameters, where new parameter = old parameter - step size\nRepeat 3 - 4 until the step size is very small or you reach a max number of steps."
  },
  {
    "objectID": "posts/daily/2025-11-29/index.html#ml-notes",
    "href": "posts/daily/2025-11-29/index.html#ml-notes",
    "title": "Daily Notes: 2025-11-29",
    "section": "",
    "text": "Gradient Descent, or how machines learn\n\nWhen measuring how “costly” a wrong prediction is / how “accurate” a training example was, you can use a loss function.\nOne example of a loss function is the Sum of Squared Errors (SSE). Formally defined as:\n\n\\(SSE = \\sum\\limits_{i} (y_i - \\hat{y}_i)^2\\)\n\n\n\n\n\n\nNoteWhat’s great about SSE?\n\n\n\n\nIt is said to be appropriate when modeling regression with Gaussian noise.\nIt is not appropriate for classification or distributions with outliers (perhaps because it penalizes large errors heavily?).\n\n\n\n\nRecall that Weights \\(w\\) represent how strongly each input dimension influences the neuron, and the Bias \\(b\\) shifts the activation threshold and acts as an offset.\n\n3Blue1Brown says: Neurons are connected to the neurons in the previous layer. The weights are the strength of those connections, and for ReLU-like activations, the bias affects when the neuron is active/inactive.\nReLU here refers to a Rectified Linear Unit, and it can be defined as \\(ReLU(a) = max(0, a)\\). Basically, the sigmoid was a slow learner and hard to train, and the ReLU made things easier by making a neuron “active” (\\(a\\)) or “inactive” (\\(0\\)).\n\nA network “learns” by adjusting the parameters \\(W\\) and \\(b\\) to minimize a cost function / loss function.\n\n\n\n\n\n\n\nNoteW vs. w\n\n\n\n\\(W\\) and \\(w\\) mean different things in neural networks.\n\\(W\\): A weight matrix for an entire layer. If a layer has \\(n_{in}\\) inputs and \\(m_{out}\\) neurons, then \\(W \\in R^{n_{in} \\times m_{out}}\\). Each row is the weight vector of one neuron, and \\(z = Wx + b\\) is the equivalent of doing many individual dot products at once.\n\\(w\\): A weight vector for a single neuron.\n\\(w = \\begin{bmatrix}\nw_1 \\\\\nw_2 \\\\\n\\vdots \\\\\nw_n\n\\end{bmatrix}\\)\n\n\nFormally, Gradient Descent is the algorithm used to minimize a loss function \\(J(\\theta)\\) by iteratively updating parameters in the direction that reduces the loss:\n\\(\\theta := \\theta - \\eta \\nabla_{\\theta} J(\\theta)\\)\n\nWhat is great about Gradient Descent is that the network can minimize this loss function by converging towards a local minimum, even when the derivative of the minimum is not 0.\nGradient Descent, by design, can take big “steps” towards the minimum when it determines that it is far away, and smaller steps when it determines that it is closer.\nThink of a “step” as the derivative of the sum of squared residuals with respect to a certain parameter, multiplied by a learning rate\nGradient Descent stops taking steps when the “step size” is very close to 0 or if it is forced to give up (i.e., it surpasses the maximum number of steps)\n\n\n\n\n\n\n\nNoteWhy is it called Gradient Descent?\n\n\n\n\nA Gradient is two or more derivatives of the same function\nA Gradient gives the direction of steepest increase (from Multivariable Calculus)\nWe use this Gradient to descend in the direction of \\(-\\nabla J(\\theta)\\) to reach the lowest point of the loss function.\n\n\n\nGradient Descent in plain English:\n\nFind the derivative of the Loss Function for each parameter in it. (i.e., take the Gradient of the Loss Function).\nPick random values for the parameters, and plug them into the Gradient.\nCalculate the step sizes, where step size == slope * learning rate\nCalculate the new parameters, where new parameter = old parameter - step size\nRepeat 3 - 4 until the step size is very small or you reach a max number of steps."
  },
  {
    "objectID": "posts/daily/2025-11-29/index.html#personal-notes",
    "href": "posts/daily/2025-11-29/index.html#personal-notes",
    "title": "Daily Notes: 2025-11-29",
    "section": "Personal Notes",
    "text": "Personal Notes\n\nGabriel Petersson said to “decide between diffusion or LLMs” when starting out with ML. It seems that modern ML has two giant “starting domains” that give you maximum learning per unit of effort. In these worlds:\n\nthe abstractions are clean\nthe tooling is mature\nthe concepts are foundational\nthe project ideas are endless\nthe market impact is real\nthe architecture teaches you almost everything else downstream\n\nIt is interesting because you can look at this as:\n\nDiffusion = generative modeling in continuous noise spaces (images, for example, are made of continuous values).\nLLMs = generative modeling in discrete token spaces (language is discrete and is generated by tokens, or integer indices in a finite vocabulary).\n\nThe final frontier for LLMs appears to be domain expertise and writing great evals. It seems like that’s what’s missing with today’s frontier models - how well does it replicate something a domain expert would consider to be good? That is the competitive advantage. More to come here (would like to experiment with interviewing domain experts and writing evals with them)."
  },
  {
    "objectID": "posts/daily/2025-11-29/index.html#questions-i-still-have",
    "href": "posts/daily/2025-11-29/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-11-29",
    "section": "Questions I still have",
    "text": "Questions I still have\n\nTo answer a question from yesterday, it does seem like MNNs are a version of the SNN where there is a \\(wx + b\\) weight & bias calculation per layer, and then you do it again and again."
  },
  {
    "objectID": "posts/daily/2025-11-29/index.html#tomorrows-plan",
    "href": "posts/daily/2025-11-29/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-11-29",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan\n\nI need to complete my understanding of Gradient Descent. I developed an intuition for it, but it is still basic."
  },
  {
    "objectID": "posts/daily/index.html",
    "href": "posts/daily/index.html",
    "title": "Daily ML Logs",
    "section": "",
    "text": "Short, lightly edited logs of what I worked on each day.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-12-21\n\n\n\n\n\n\n\n\nDec 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-12-20\n\n\n\n\n\n\n\n\nDec 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-12-14\n\n\n\n\n\n\n\n\nDec 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-12-13\n\n\n\n\n\n\n\n\nDec 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-12-11\n\n\n\n\n\n\n\n\nDec 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-12-10\n\n\n\n\n\n\n\n\nDec 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-12-08\n\n\n\n\n\n\n\n\nDec 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-29\n\n\n\n\n\n\n\n\nNov 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-28\n\n\n\n\n\n\n\n\nNov 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-27\n\n\n\n\n\n\n\n\nNov 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-19\n\n\n\n\n\n\n\n\nNov 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-18\n\n\n\n\n\n\n\n\nNov 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-16\n\n\n\n\n\n\n\n\nNov 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-15\n\n\n\n\n\n\n\n\nNov 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDaily Notes: 2025-11-14\n\n\n\n\n\n\n\n\nNov 14, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/daily/2025-12-21/index.html",
    "href": "posts/daily/2025-12-21/index.html",
    "title": "Daily Notes: 2025-12-21",
    "section": "",
    "text": "Logistic Regression\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(x: np.ndarray) -&gt; np.ndarray:\n    return 1 / (1 + np.exp(-x))\n\nx = np.linspace(-10, 10, 400)\ny = sigmoid(x)\n\nplt.figure(figsize=(6, 4))\nplt.plot(x, y, label=r'$\\sigma(x) = \\frac{1}{1 + e^{-x}}$')\nplt.axvline(0, color='gray', lw=0.7, ls='--')\nplt.axhline(0.5, color='gray', lw=0.7, ls='--')\nplt.scatter([0], [0.5], color='red', zorder=5)\nplt.text(0.5, 0.5, 'threshold', va='center')\nplt.xlabel('x')\nplt.ylabel('σ(x)')\nplt.title('Sigmoid Function')\nplt.legend()\nplt.grid(True, ls='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nConsider the sigmoid function, which is popular within AI because it squashes \\(x\\) to be a number between 0 and 1.\n\\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)\nConsider the weighted sum (aka dot product) which is:\n\\(\\theta^{T}x = \\sum_{i=1}^{m}\\theta_{i}x_{i}\\)\nTherefore the sigmoid function of the weighted sum is:\n\\(\\sigma(\\theta^{T}x) = \\frac{1}{1 + e^{-\\theta^{T}x}}\\)\n\nThe “lightbulb idea” behind logistic regression is predicting \\(P(X | Y)\\) directly (more rigorously than a Naive Bayes Classifier).\nLogistic regression predicts discrete (e.g., True vs. False) rather than continuous values. It fits a function resembling a sigmoid function to the data.\nInstead of least squares as a cost function, it uses maximum likelihood"
  },
  {
    "objectID": "posts/daily/2025-12-21/index.html#ml-notes",
    "href": "posts/daily/2025-12-21/index.html#ml-notes",
    "title": "Daily Notes: 2025-12-21",
    "section": "",
    "text": "Logistic Regression\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(x: np.ndarray) -&gt; np.ndarray:\n    return 1 / (1 + np.exp(-x))\n\nx = np.linspace(-10, 10, 400)\ny = sigmoid(x)\n\nplt.figure(figsize=(6, 4))\nplt.plot(x, y, label=r'$\\sigma(x) = \\frac{1}{1 + e^{-x}}$')\nplt.axvline(0, color='gray', lw=0.7, ls='--')\nplt.axhline(0.5, color='gray', lw=0.7, ls='--')\nplt.scatter([0], [0.5], color='red', zorder=5)\nplt.text(0.5, 0.5, 'threshold', va='center')\nplt.xlabel('x')\nplt.ylabel('σ(x)')\nplt.title('Sigmoid Function')\nplt.legend()\nplt.grid(True, ls='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nConsider the sigmoid function, which is popular within AI because it squashes \\(x\\) to be a number between 0 and 1.\n\\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)\nConsider the weighted sum (aka dot product) which is:\n\\(\\theta^{T}x = \\sum_{i=1}^{m}\\theta_{i}x_{i}\\)\nTherefore the sigmoid function of the weighted sum is:\n\\(\\sigma(\\theta^{T}x) = \\frac{1}{1 + e^{-\\theta^{T}x}}\\)\n\nThe “lightbulb idea” behind logistic regression is predicting \\(P(X | Y)\\) directly (more rigorously than a Naive Bayes Classifier).\nLogistic regression predicts discrete (e.g., True vs. False) rather than continuous values. It fits a function resembling a sigmoid function to the data.\nInstead of least squares as a cost function, it uses maximum likelihood"
  },
  {
    "objectID": "posts/daily/2025-12-21/index.html#personal-notes",
    "href": "posts/daily/2025-12-21/index.html#personal-notes",
    "title": "Daily Notes: 2025-12-21",
    "section": "Personal Notes",
    "text": "Personal Notes"
  },
  {
    "objectID": "posts/daily/2025-12-21/index.html#questions-i-still-have",
    "href": "posts/daily/2025-12-21/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-12-21",
    "section": "Questions I still have",
    "text": "Questions I still have"
  },
  {
    "objectID": "posts/daily/2025-12-21/index.html#tomorrows-plan",
    "href": "posts/daily/2025-12-21/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-12-21",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan"
  },
  {
    "objectID": "posts/daily/2025-12-11/index.html",
    "href": "posts/daily/2025-12-11/index.html",
    "title": "Daily Notes: 2025-12-11",
    "section": "",
    "text": "Introduction to Classification (Using scikit-learn)\n\nGiven the “no free lunch theorem,” it is always recommended to try a handful of different learning algorithms to choose the best for a particular problem.\nFactors include:\n\nThe amount of training examples or features\nThe amount of noise in the dataset\nWhether the classes are linearly separable\n\n\n\n\n\nSelecting features + collecting (labeled) training examples.\nChoosing a performance metric.\nChoosing a learning algorithm + training a model.\nEvaluating the performance based on the performance metric.\nChanging the settings of the algorithm + tuning the model. (Adjusting hyperparameters like learning rate, # of epochs, etc.)"
  },
  {
    "objectID": "posts/daily/2025-12-11/index.html#ml-notes",
    "href": "posts/daily/2025-12-11/index.html#ml-notes",
    "title": "Daily Notes: 2025-12-11",
    "section": "",
    "text": "Introduction to Classification (Using scikit-learn)\n\nGiven the “no free lunch theorem,” it is always recommended to try a handful of different learning algorithms to choose the best for a particular problem.\nFactors include:\n\nThe amount of training examples or features\nThe amount of noise in the dataset\nWhether the classes are linearly separable\n\n\n\n\n\nSelecting features + collecting (labeled) training examples.\nChoosing a performance metric.\nChoosing a learning algorithm + training a model.\nEvaluating the performance based on the performance metric.\nChanging the settings of the algorithm + tuning the model. (Adjusting hyperparameters like learning rate, # of epochs, etc.)"
  },
  {
    "objectID": "posts/daily/2025-12-11/index.html#personal-notes",
    "href": "posts/daily/2025-12-11/index.html#personal-notes",
    "title": "Daily Notes: 2025-12-11",
    "section": "Personal Notes",
    "text": "Personal Notes"
  },
  {
    "objectID": "posts/daily/2025-12-11/index.html#questions-i-still-have",
    "href": "posts/daily/2025-12-11/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-12-11",
    "section": "Questions I still have",
    "text": "Questions I still have"
  },
  {
    "objectID": "posts/daily/2025-12-11/index.html#tomorrows-plan",
    "href": "posts/daily/2025-12-11/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-12-11",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan"
  },
  {
    "objectID": "posts/daily/2025-11-15/index.html",
    "href": "posts/daily/2025-11-15/index.html",
    "title": "Daily Notes: 2025-11-15",
    "section": "",
    "text": "A Mental Map for ML\n\nMachine Learning: Broad umbrella\nDeep Learning: Subset of ML using multi-layer neural networks\nTransformer architectures: Specific type of neural network architecture introduced in Attention Is All You Need (Vaswani et al., 2017).\nLLMs: a large transformer architecture, using a large corpus of text.\n\nThe importance of Linear Algebra\n\nLinear Algebra is the language of ML because ML is fundamentally the art of taking something (image, soundwave, text, etc.), representing it as a vector (or matrix), and creating a model that transforms that vector into another vector.\nAs an example, for a neural network layer, \\(y = Wx + b\\) represents \\(y\\) the output vector, \\(W\\) the learned weights (as a matrix), \\(x\\) the input vector, and \\(b\\) the bias vector.\nLinear Algebra permits us not only a compact representation, but also a language to reason about and manipulate these representations.\n\nThe importance of Probability\n\nThe guiding principle here is that some branches of computer science deal with entities that are deterministic and certain, but ML deals with entities that are stochastic (nondeterministic) and uncertain.\nProbability is how you reason in the presence of uncertainty. Interestingly, uncertainty is more common than we think… how many propositions are guaranteed to be true, or events are guaranteed to occur?\nThree possibile sources of uncertainty according to Goodfellow:\n\nInherent stochasticity in the system being modeled\nIncomplete observability: Even in a deterministic system, can’t observe all the variables that drive behavior (e.g., Monty Hall problem)\nIncomplete modeling: This one is the most interesting. A model must sometimes discard observed information. Sometimes, it’s more practical to use a simple & uncertain rule than a complex & certain & deterministic rule (e.g., “most birds fly” is better than “birds fly, except…”). It’s hard to develop, communicate, maintain, and make it resilient against failure.\n\n\nMiscellaneous\n\nSurprisingly, according to ChatGPT as an answer to one of my questions yesterday, Reinforcement Learning is typically “harder” than Unsupervised Learning.\n\nSupervised Learning is the “easiest” because you have clear labels, a clear objective, and a clear way of measuring success (goal: minimize loss).\nUnsupervised Learning is “conceptually tricky, computationally easy.” Yes, there’s no objective ground truth, but optimization is “stable” (i.e., the loss function is well-behaved, gradient descent makes progress reliably, feedback is consistent). Examples: PCA, k-means, etc.\nReinforcement Learning is hard because the environment is stochastic, the feedback is often delayed/sparse, and the optimization is unstable.\n\nTo answer another question from yesterday, here is the meaning of ““If you compute parameters for feature scaling or dimensionality reduction using all the data (train + test), the test performance becomes overly optimistic.”: your model has seen structures of the test data before evaluation, which means that there was leakage. The test set is supposed to simulate new, unseen, real-world data, and so it underestimates the “true error” that the model would have.\n\nRendering Python using Quarto…\n\nimport numpy as np \n\nx = np.arange(5)\nx\n\narray([0, 1, 2, 3, 4])"
  },
  {
    "objectID": "posts/daily/2025-11-15/index.html#ml-notes",
    "href": "posts/daily/2025-11-15/index.html#ml-notes",
    "title": "Daily Notes: 2025-11-15",
    "section": "",
    "text": "A Mental Map for ML\n\nMachine Learning: Broad umbrella\nDeep Learning: Subset of ML using multi-layer neural networks\nTransformer architectures: Specific type of neural network architecture introduced in Attention Is All You Need (Vaswani et al., 2017).\nLLMs: a large transformer architecture, using a large corpus of text.\n\nThe importance of Linear Algebra\n\nLinear Algebra is the language of ML because ML is fundamentally the art of taking something (image, soundwave, text, etc.), representing it as a vector (or matrix), and creating a model that transforms that vector into another vector.\nAs an example, for a neural network layer, \\(y = Wx + b\\) represents \\(y\\) the output vector, \\(W\\) the learned weights (as a matrix), \\(x\\) the input vector, and \\(b\\) the bias vector.\nLinear Algebra permits us not only a compact representation, but also a language to reason about and manipulate these representations.\n\nThe importance of Probability\n\nThe guiding principle here is that some branches of computer science deal with entities that are deterministic and certain, but ML deals with entities that are stochastic (nondeterministic) and uncertain.\nProbability is how you reason in the presence of uncertainty. Interestingly, uncertainty is more common than we think… how many propositions are guaranteed to be true, or events are guaranteed to occur?\nThree possibile sources of uncertainty according to Goodfellow:\n\nInherent stochasticity in the system being modeled\nIncomplete observability: Even in a deterministic system, can’t observe all the variables that drive behavior (e.g., Monty Hall problem)\nIncomplete modeling: This one is the most interesting. A model must sometimes discard observed information. Sometimes, it’s more practical to use a simple & uncertain rule than a complex & certain & deterministic rule (e.g., “most birds fly” is better than “birds fly, except…”). It’s hard to develop, communicate, maintain, and make it resilient against failure.\n\n\nMiscellaneous\n\nSurprisingly, according to ChatGPT as an answer to one of my questions yesterday, Reinforcement Learning is typically “harder” than Unsupervised Learning.\n\nSupervised Learning is the “easiest” because you have clear labels, a clear objective, and a clear way of measuring success (goal: minimize loss).\nUnsupervised Learning is “conceptually tricky, computationally easy.” Yes, there’s no objective ground truth, but optimization is “stable” (i.e., the loss function is well-behaved, gradient descent makes progress reliably, feedback is consistent). Examples: PCA, k-means, etc.\nReinforcement Learning is hard because the environment is stochastic, the feedback is often delayed/sparse, and the optimization is unstable.\n\nTo answer another question from yesterday, here is the meaning of ““If you compute parameters for feature scaling or dimensionality reduction using all the data (train + test), the test performance becomes overly optimistic.”: your model has seen structures of the test data before evaluation, which means that there was leakage. The test set is supposed to simulate new, unseen, real-world data, and so it underestimates the “true error” that the model would have.\n\nRendering Python using Quarto…\n\nimport numpy as np \n\nx = np.arange(5)\nx\n\narray([0, 1, 2, 3, 4])"
  },
  {
    "objectID": "posts/daily/2025-11-15/index.html#personal-notes",
    "href": "posts/daily/2025-11-15/index.html#personal-notes",
    "title": "Daily Notes: 2025-11-15",
    "section": "Personal Notes",
    "text": "Personal Notes\nI finally learned the differences between pip, Anaconda, and Miniconda today.\n\npip is a lightweight universal package installer that can install python libraries (e.g., numpy) pretty much anywhere. It only installs packages from PyPI.\nAnaconda is a (huge… 3-5 GB) Python distribution + environment manager - it bundles everything (pip, pre-compiled scientific packages, etc.) in one. Its scope goes beyond PyPI.\nMiniconda is a lighter version of Anaconda. It has the environment management that pip doesn’t have by default, and Python, but nothing else out the gate. This is important because you can manage your own Python versions in isolated environments.\nRecommendation: Use Miniconda for environment management + Python version control, use pip within each conda environment for packages.\nI created an environment using miniconda called pyml with the following packages: NumPy 1.21.2 SciPy 1.7.0 Scikit-learn 1.0 Matplotlib 3.4.3 pandas 1.3.2 python 3.9\n\nA “Karpathy-style” learning math on demand roadmap\n\nNote: Linear Algebra, followed by Probability, followed by Calculus are the most important\n\n\nStart with a tiny ML/DL problem first (e.g., train a linear classifier or a 2-layer neural net) You will immediately run into the following math concepts that need refreshing:\n\n\ngradients → calc\nmatrix multiplies → Lin Alg\nloss functions → probability + info theory\noptimization steps → calc + LA\nembeddings → SVD intuition\n\n\nWhen you start reading transformers papers, refresh only:\n\n\ndot products\nmatrix multiplications\nsoftmax derivatives\neigen decomposition (for understanding attention as soft nearest-neighbor search)\nprobability for next-token prediction\n\n\nWhen you study self-supervised methods, refresh only:\n\n\nKL divergence\ncross-entropy\ncovariance\nGaussians\ncontrastive loss geometry\n\n\nWhen you start fine-tuning LLMs, refresh only:\n\n\ngradients of logits\nJacobians\nbasic linear algebra on embeddings\nmatrix factorization intuition"
  },
  {
    "objectID": "posts/daily/2025-11-15/index.html#questions-i-still-have",
    "href": "posts/daily/2025-11-15/index.html#questions-i-still-have",
    "title": "Daily Notes: 2025-11-15",
    "section": "Questions I still have",
    "text": "Questions I still have\n\nN/A as of now"
  },
  {
    "objectID": "posts/daily/2025-11-15/index.html#tomorrows-plan",
    "href": "posts/daily/2025-11-15/index.html#tomorrows-plan",
    "title": "Daily Notes: 2025-11-15",
    "section": "Tomorrow’s plan",
    "text": "Tomorrow’s plan\n\nActually implement something"
  },
  {
    "objectID": "posts/concepts/index.html",
    "href": "posts/concepts/index.html",
    "title": "Concepts",
    "section": "",
    "text": "More polished explanations of ML concepts that I want to be able to reference later.\n\n\n\n\n\n\n\n\nNo matching items"
  }
]