{
  "hash": "484ba282ad21adef43b4315bd03e41e9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Daily Notes: 2025-11-15\"\ndate: 2025-11-15\ncategories: [daily]\n---\n\n\nML Notes\n- \nA Mental Map for ML\n\n- Machine Learning: Broad umbrella\n- Deep Learning: Subset of ML using multi-layer neural networks\n- Transformer architectures: Specific type of neural network architecture introduced in *Attention Is All You Need (Vaswani et al., 2017).*\n- LLMs: a large transformer architecture, using a large corpus of text.\n\nThe importance of Linear Algebra\n\n- Linear Algebra is the language of ML because ML is fundamentally the art of taking something (image, soundwave, text, etc.), representing it as a **vector** (or matrix), and creating a model that transforms that vector into another vector.\n- As an example, for a neural network layer, $y = Wx + b$ represents $y$ the output vector, $W$ the learned weights (as a matrix), $x$ the input vector,  and $b$ the bias vector.\n- Linear Algebra permits us not only a compact representation, but also a language to reason about and manipulate these representations.\n\nThe importance of Probability\n\n- The guiding principle here is that some branches of computer science deal with entities that are **deterministic** and certain, but ML deals with entities that are **stochastic** (nondeterministic) and uncertain.\n- Probability is how you reason in the presence of uncertainty. Interestingly, uncertainty is more common than we think... how many propositions are guaranteed to be true, or events are guaranteed to occur?\n- Three possibile sources of uncertainty according to [Goodfellow](https://www.deeplearningbook.org):\n    - Inherent stochasticity in the system being modeled\n    - Incomplete observability: Even in a deterministic system, can't observe all the variables that drive behavior (e.g., Monty Hall problem)\n    - Incomplete modeling: This one is the most interesting. *A model must sometimes discard observed information.* Sometimes, it's more practical to use a simple & uncertain rule than a complex & certain & deterministic rule (e.g., \"most birds fly\" is better than \"birds fly, except...\"). It's hard to develop, communicate, maintain, and make it resilient against failure.\n\nMiscellaneous\n\n- Surprisingly, according to ChatGPT as an answer to one of my questions yesterday, **Reinforcement Learning** is typically \"harder\" than **Unsupervised Learning**.\n    - **Supervised Learning** is the \"easiest\" because you have clear labels, a clear objective, and a clear way of measuring success (goal: minimize loss).\n    - **Unsupervised Learning** is \"conceptually tricky, computationally easy.\" Yes, there's no objective ground truth, but optimization is \"stable\" (i.e., the loss function is well-behaved, gradient descent makes progress reliably, feedback is consistent). Examples: PCA, k-means, etc.\n    - **Reinforcement Learning** is hard because the environment is stochastic, the feedback is often delayed/sparse, and the optimization is unstable.\n- To answer another question from yesterday, here is the meaning of \"“If you compute parameters for feature scaling or dimensionality reduction using all the data (train + test), the test performance becomes overly optimistic.\": your model has seen structures of the test data before evaluation, which means that there was **leakage**. The test set is supposed to simulate new, unseen, real-world data, and so it underestimates the \"true error\" that the model would have.\n\nRendering Python using Quarto...\n\n::: {#a9be49bd .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np \n\nx = np.arange(5)\nx\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\narray([0, 1, 2, 3, 4])\n```\n:::\n:::\n\n\nPersonal Notes\n-\nI finally learned the differences between `pip`, Anaconda, and Miniconda today. \n\n- `pip` is a lightweight universal package installer that can install python libraries (e.g., `numpy`) pretty much anywhere. It only installs packages from PyPI.\n- Anaconda is a (huge... 3-5 GB) Python distribution + environment manager - it bundles everything (`pip`, pre-compiled scientific packages, etc.) in one. Its scope goes beyond PyPI.\n- Miniconda is a lighter version of Anaconda. It has the environment management that `pip` doesn't have by default, and Python, but nothing else out the gate. This is important because you can manage your own Python versions in isolated environments.\n- *Recommendation*: Use Miniconda for environment management + Python version control, use `pip` within each conda environment for packages.\n- I created an environment using miniconda called `pyml` with the following packages: NumPy 1.21.2 SciPy 1.7.0 Scikit-learn 1.0 Matplotlib 3.4.3 pandas 1.3.2 python 3.9\n\nA \"Karpathy-style\" learning math on demand roadmap\n\n- Note: Linear Algebra, followed by Probability, followed by Calculus are the most important\n\n1. Start with a tiny ML/DL problem first (e.g., train a linear classifier or a 2-layer neural net)\nYou will immediately run into the following math concepts that need refreshing:\n\n- gradients → calc\n- matrix multiplies → Lin Alg\n- loss functions → probability + info theory\n- optimization steps → calc + LA\n- embeddings → SVD intuition\n\n\n2. When you start reading transformers papers, refresh only:\n\n- dot products\n- matrix multiplications\n- softmax derivatives\n- eigen decomposition (for understanding attention as soft nearest-neighbor search)\n- probability for next-token prediction\n\n3. When you study self-supervised methods, refresh only:\n\n- KL divergence\n- cross-entropy\n- covariance\n- Gaussians\n- contrastive loss geometry\n\n4. When you start fine-tuning LLMs, refresh only:\n\n- gradients of logits\n- Jacobians\n- basic linear algebra on embeddings\n- matrix factorization intuition\n\nQuestions I still have\n- \n-\n\n\nTomorrow's plan\n- \n- \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}